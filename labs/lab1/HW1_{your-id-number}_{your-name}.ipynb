{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a5a31d058a6316b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 作业一：实现HMM中文分词和BPE英文分词\n",
    "姓名：陈永俊\n",
    "\n",
    "学号：522031910203"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7a16ed",
   "metadata": {},
   "source": [
    "## 任务一：HMM模型用于中文分词\n",
    "\n",
    "任务一评分标准：\n",
    "1. 共有8处TODO需要填写，每个TODO计1-2分，共9分，预计代码量30行。\n",
    "2. 允许自行修改、编写代码完成。对于该情况，请补充注释以便于评分，否则结果不正确将导致较多的扣分。\n",
    "3. 用于说明实验的文字和总结不额外计分，但不写会导致扣分。\n",
    "\n",
    "注：本任务仅在短句子上进行效果测试，因此对概率的计算可直接进行连乘。在实践中，常先对概率取对数，将连乘变为加法来计算，以避免出现数值溢出的情况。\n",
    "\n",
    "> 你可以像这样在Markdown单元格中使用引用符号`>`，  \n",
    "以及在代码单元格中使用注释来说明你的实验。  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "5fc4dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d77db9",
   "metadata": {},
   "source": [
    "导入HMM参数，初始化所需的起始概率矩阵、转移概率矩阵、发射概率矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "0d25beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hmm_parameters.pkl\", \"rb\") as f:\n",
    "    hmm_parameters = pickle.load(f)\n",
    "\n",
    "# 非断字（B）为第0行，断字（I）为第1行\n",
    "# 发射概率矩阵中，词典大小为65536，以汉字的Unicode码点（一个整数值）作为行索引\n",
    "start_probability = hmm_parameters[\"start_prob\"]  # shape(2,)\n",
    "trans_matrix = hmm_parameters[\"trans_mat\"]  # shape(2, 2)\n",
    "emission_matrix = hmm_parameters[\"emission_mat\"]  # shape(2, 65536)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7070152",
   "metadata": {},
   "source": [
    "定义待处理的句子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "87219e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 将your_name中的xxx替换为你的姓名【1分】\n",
    "your_name = \"陈永俊\"\n",
    "input_sentence = f\"{your_name}是一名优秀的学生\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1035cbc7",
   "metadata": {},
   "source": [
    "实现Viterbi算法，并以此进行中文分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "1adac849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sent_orig: str, start_prob: np.ndarray, trans_mat: np.ndarray, emission_mat: np.ndarray) -> str:\n",
    "    \"\"\"\n",
    "    Viterbi算法进行中文分词。\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - 输入的句子\n",
    "        start_prob: numpy.ndarray - 起始概率矩阵\n",
    "        trans_mat: numpy.ndarray - 转移概率矩阵\n",
    "        emission_mat: numpy.ndarray - 发射概率矩阵\n",
    "\n",
    "    Return:\n",
    "        str - 中文分词的结果\n",
    "    \"\"\"\n",
    "\n",
    "    #  将汉字转为数字表示\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "\n",
    "    # `dp`用来储存不同位置每种标注（B/I）的最大概率值\n",
    "    dp = np.zeros((2, len(sent_ord)), dtype=float)\n",
    "\n",
    "    # `path`用来储存最大概率对应的上步B/I选择\n",
    "    #  例如 path[1][7] == 1 意味着第8个（从1开始计数）字符标注I对应的最大概率，其前一步的隐状态为1（I）\n",
    "    #  例如 path[0][5] == 1 意味着第6个字符标注B对应的最大概率，其前一步的隐状态为1（I）\n",
    "    #  例如 path[1][1] == 0 意味着第2个字符标注I对应的最大概率，其前一步的隐状态为0（B）\n",
    "    path = np.zeros((2, len(sent_ord)), dtype=int)\n",
    "\n",
    "    #  TODO: 第一个位置的最大概率值计算【1分】\n",
    "    pass\n",
    "    dp[0][0] = start_prob[0]*emission_mat[0][sent_ord[0]]\n",
    "    dp[1][0] = start_prob[1]*emission_mat[1][sent_ord[0]]\n",
    "    # argmax \n",
    "    #  TODO: 其余位置的最大概率值计算（填充dp和path矩阵）【2分】\n",
    "    pass\n",
    "    # path[i][j] = argmax_{k} dp[i][j]\n",
    "    # dp[i][j] = max_{k}(dp[k][j-1]*trans_mat[k][i]*emission_mat[i][sent_ord[j]])\n",
    "    # for i in range(1,len(sent_ord)):\n",
    "    for j in range(1, len(sent_ord)):\n",
    "        for i in range(0,2):\n",
    "            max_prob, max_state = max(\n",
    "                (dp[k][j-1] * trans_mat[k][i] * emission_mat[i][sent_ord[j]], k) for k in range(2)\n",
    "            )\n",
    "            dp[i][j] = max_prob\n",
    "            path[i][j] = max_state\n",
    "    #  `labels`用来储存每个位置最有可能的隐状态\n",
    "    labels = [0 for _ in range(len(sent_ord))]\n",
    "\n",
    "    #  TODO: 计算labels每个位置上的值（填充labels矩阵）【1分】\n",
    "    pass\n",
    "    # 从最后一个位置开始回溯\n",
    "    labels[-1] = np.argmax(dp[:,-1])\n",
    "    for j in range(len(sent_ord)-1,0,-1):\n",
    "        labels[j-1] = path[labels[j]][j]\n",
    "    #  根据lalels生成切分好的字符串\n",
    "    sent_split = []\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label == 1:\n",
    "            sent_split += [sent_ord[idx], ord(\"/\")]\n",
    "        else:\n",
    "            sent_split += [sent_ord[idx]]\n",
    "    sent_split_str = \"\".join([chr(x) for x in sent_split])\n",
    "\n",
    "    return sent_split_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "d795414b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi算法分词结果： 陈/永俊是/一名/优秀/的/学生/\n"
     ]
    }
   ],
   "source": [
    "print(\"Viterbi算法分词结果：\", viterbi(input_sentence, start_probability, trans_matrix, emission_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fcafdb",
   "metadata": {},
   "source": [
    "实现前向算法，计算该句子的概率值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "cf6796a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prob_by_forward(sent_orig: str, start_prob: np.ndarray, trans_mat: np.ndarray, emission_mat: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    前向算法，计算输入中文句子的概率值。\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - 输入的句子\n",
    "        start_prob: numpy.ndarray - 起始概率矩阵\n",
    "        trans_mat: numpy.ndarray - 转移概率矩阵\n",
    "        emission_mat: numpy.ndarray - 发射概率矩阵\n",
    "\n",
    "    Return:\n",
    "        float - 概率值\n",
    "    \"\"\"\n",
    "\n",
    "    #  将汉字转为数字表示\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "\n",
    "    # `dp`用来储存不同位置每种隐状态（B/I）下，到该位置为止的句子的概率\n",
    "    dp = np.zeros((2, len(sent_ord)), dtype=float)\n",
    "\n",
    "    # TODO: 初始位置概率的计算【1分】\n",
    "    pass\n",
    "    dp[0][0] = start_prob[0]*emission_mat[0][sent_ord[0]]\n",
    "    dp[1][0] = start_prob[1]*emission_mat[1][sent_ord[0]]\n",
    "    # TODO: 先计算其余位置的概率（填充dp矩阵），然后返回概率值【1分】\n",
    "    pass\n",
    "    for i in range (1,len(sent_ord)):\n",
    "        \n",
    "            dp[0][i] = sum([dp[k][i-1] * trans_mat[k][0] * emission_mat[0][sent_ord[i]] for k in range(2)])\n",
    "            dp[1][i] = sum([dp[k][i-1] * trans_mat[k][1] * emission_mat[1][sent_ord[i]] for k in range(2)])\n",
    "    return sum([dp[i][len(sent_ord) - 1] for i in range(2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59533cd8",
   "metadata": {},
   "source": [
    "实现后向算法，计算该句子的概率值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "1e898306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prob_by_backward(sent_orig: str, start_prob: np.ndarray, trans_mat: np.ndarray, emission_mat: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    后向算法，计算输入中文句子的概率值。\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - 输入的句子\n",
    "        start_prob: numpy.ndarray - 起始概率矩阵\n",
    "        trans_mat: numpy.ndarray - 转移概率矩阵\n",
    "        emission_mat: numpy.ndarray - 发射概率矩阵\n",
    "\n",
    "    Return:\n",
    "        float - 概率值\n",
    "    \"\"\"\n",
    "\n",
    "    #  将汉字转为数字表示\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "\n",
    "    # `dp`用来储存不同位置每种隐状态（B/I）下，从结尾到该位置为止的句子的概率\n",
    "    dp = np.zeros((2, len(sent_ord)), dtype=float)\n",
    "\n",
    "    # TODO: 终末位置概率的初始化【1分】\n",
    "    pass\n",
    "    dp[0][len(sent_ord)-2] = sum([1 * trans_mat[0][i] * emission_mat[i][sent_ord[len(sent_ord)-1]] for i in range(2)])\n",
    "    dp[1][len(sent_ord)-2] = sum([1 * trans_mat[1][i] * emission_mat[i][sent_ord[len(sent_ord)-1]] for i in range(2)])\n",
    "    # TODO: 先计算其余位置的概率（填充dp矩阵），然后返回概率值【1分】\n",
    "    pass\n",
    "    for k in range(len(sent_ord)-3,-1,-1):\n",
    "        dp[0][k] = sum([dp[i][k+1]*trans_mat[0][i]*emission_mat[i][sent_ord[k+1]] for i in range(2)])\n",
    "        dp[1][k] = sum([dp[i][k+1]*trans_mat[1][i]*emission_mat[i][sent_ord[k+1]] for i in range(2)])\n",
    "    return sum([dp[i][0] * start_prob[i] * emission_mat[i][sent_ord[0]] for i in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "b26101d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前向算法概率： 4.857127292636688e-34\n",
      "后向算法概率： 4.857127292636689e-34\n"
     ]
    }
   ],
   "source": [
    "print(\"前向算法概率：\", compute_prob_by_forward(input_sentence, start_probability, trans_matrix, emission_matrix))\n",
    "print(\"后向算法概率：\", compute_prob_by_backward(input_sentence, start_probability, trans_matrix, emission_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20803f6a1a465dd6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 实验总结\n",
    "> TODO：请在这里填写实验总结。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e994be6e",
   "metadata": {},
   "source": [
    "## 任务二：BPE算法用于英文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cc4775",
   "metadata": {},
   "source": [
    "任务二评分标准：\n",
    "\n",
    "1. 共有6处TODO需要填写，每个TODO计1-2分，共9分，预计代码量50行。\n",
    "2. 允许自行修改、编写代码完成。对于该情况，请补充注释以便于评分，否则结果不正确将导致较多的扣分。\n",
    "3. 用于说明实验的文字和总结不额外计分，但不写会导致扣分。\n",
    "\n",
    "> 你可以像这样在Markdown单元格中使用引用符号`>`，  \n",
    "以及在代码单元格中使用注释来说明你的实验。  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "f02463b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5dbb9",
   "metadata": {},
   "source": [
    "构建空格分词器，将语料中的句子以空格切分成单词，然后将单词拆分成字母加`</w>`的形式。例如`apple`将变为`a p p l e </w>`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "d6c3667a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'am', 'happy'], ['i', 'have', 'N', 'apples']]\n"
     ]
    }
   ],
   "source": [
    "_splitor_pattern = re.compile(r\"[^a-zA-Z']+|(?=')\")\n",
    "_digit_pattern = re.compile(r\"\\d+\")\n",
    "\n",
    "\n",
    "def white_space_tokenize(corpus: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    先正则化（字母转小写、数字转为N、除去标点符号），然后以空格分词语料中的句子，例如：  \n",
    "    输入 `corpus = [\"I am happy.\", \"I have 10 apples!\"]`，  \n",
    "    得到 `[[\"i\", \"am\", \"happy\"], [\"i\", \"have\", \"N\", \"apples\"]]`\n",
    "\n",
    "    Args:\n",
    "        corpus: List[str] - 待处理的语料\n",
    "\n",
    "    Return:\n",
    "        List[List[str]] - 二维List，内部的List由每个句子的单词str构成\n",
    "    \"\"\"\n",
    "\n",
    "    tokeneds = [list(filter(lambda token: len(token) > 0, _splitor_pattern.split(_digit_pattern.sub(\"N\", sentence.lower())))) for sentence in corpus]\n",
    "\n",
    "    return tokeneds\n",
    "\n",
    "corpus = [\"I am happy.\", \"I have 10 apples!\"]\n",
    "print(white_space_tokenize(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3732502a",
   "metadata": {},
   "source": [
    "编写相应函数构建BPE算法需要用到的初始状态词典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "7bf823e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i </w>': 2, 'a m </w>': 1, 'h a p p y </w>': 1, 'h a v e </w>': 1, 'N </w>': 1, 'a p p l e s </w>': 1}\n"
     ]
    }
   ],
   "source": [
    "def build_bpe_vocab(corpus: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    将语料进行white_space_tokenize处理后，将单词每个字母以空格隔开、结尾加上</w>后，构建带频数的字典，例如：  \n",
    "    输入 `corpus = [\"I am happy.\", \"I have 10 apples!\"]`，  \n",
    "    得到\n",
    "    ```python\n",
    "    {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        corpus: List[str] - 待处理的语料\n",
    "\n",
    "    Return:\n",
    "        Dict[str, int] - \"单词分词状态->频数\"的词典\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_corpus = white_space_tokenize(corpus)\n",
    "\n",
    "    bpe_vocab = dict()\n",
    "\n",
    "    # TODO: 完成函数体【1分】\n",
    "    for wordlist in tokenized_corpus:\n",
    "        for word in wordlist:\n",
    "            bpe_word = ' '.join(list(word))+' </w>'\n",
    "            if bpe_word in bpe_vocab:\n",
    "                bpe_vocab[bpe_word] += 1\n",
    "            else:\n",
    "                bpe_vocab[bpe_word] = 1\n",
    "    return bpe_vocab\n",
    "\n",
    "corpus = [\"I am happy.\", \"I have 10 apples!\"]\n",
    "print(build_bpe_vocab(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d25245",
   "metadata": {},
   "source": [
    "编写所需的其他函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "087d11e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('i', '</w>'): 2, ('a', 'm'): 1, ('m', '</w>'): 1, ('h', 'a'): 2, ('a', 'p'): 2, ('p', 'p'): 2, ('p', 'y'): 1, ('y', '</w>'): 1, ('a', 'v'): 1, ('v', 'e'): 1, ('e', '</w>'): 1, ('N', '</w>'): 1, ('p', 'l'): 1, ('l', 'e'): 1, ('e', 's'): 1, ('s', '</w>'): 1}\n"
     ]
    }
   ],
   "source": [
    "def get_bigram_freq(bpe_vocab: Dict[str, int]) -> Dict[Tuple[str, str], int]:\n",
    "    \"\"\"\n",
    "    统计\"单词分词状态->频数\"的词典中，各bigram的频次（假设该词典中，各个unigram以空格间隔），例如：  \n",
    "    输入 \n",
    "    ```python\n",
    "    bpe_vocab = {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "    得到\n",
    "    ```python\n",
    "    {\n",
    "        ('i', '</w>'): 2,\n",
    "        ('a', 'm'): 1,\n",
    "        ('m', '</w>'): 1,\n",
    "        ('h', 'a'): 2,\n",
    "        ('a', 'p'): 2,\n",
    "        ('p', 'p'): 2,\n",
    "        ('p', 'y'): 1,\n",
    "        ('y', '</w>'): 1,\n",
    "        ('a', 'v'): 1,\n",
    "        ('v', 'e'): 1,\n",
    "        ('e', '</w>'): 1,\n",
    "        ('N', '</w>'): 1,\n",
    "        ('p', 'l'): 1,\n",
    "        ('l', 'e'): 1,\n",
    "        ('e', 's'): 1,\n",
    "        ('s', '</w>'): 1\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        bpe_vocab: Dict[str, int] - \"单词分词状态->频数\"的词典\n",
    "\n",
    "    Return:\n",
    "        Dict[Tuple[str, str], int] - \"bigram->频数\"的词典\n",
    "    \"\"\"\n",
    "\n",
    "    bigram_freq = dict()\n",
    "\n",
    "    # TODO: 完成函数体【1分】\n",
    "    pass\n",
    "    for word in bpe_vocab:\n",
    "        tokens = word.split()  # 将单词按空格分词\n",
    "        for i in range(len(tokens) - 1):  # 遍历每个 bigram\n",
    "            bigram = (tokens[i], tokens[i + 1])\n",
    "            if bigram in bigram_freq:\n",
    "                bigram_freq[bigram] += bpe_vocab[word]\n",
    "            else:\n",
    "                bigram_freq[bigram] = bpe_vocab[word]\n",
    "    return bigram_freq\n",
    "\n",
    "bpe_vocab = {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "print(get_bigram_freq(bpe_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "ba426043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i </w>': 2, 'a m </w>': 1, 'ha p p y </w>': 1, 'ha v e </w>': 1, 'N </w>': 1, 'a p p l e s </w>': 1}\n"
     ]
    }
   ],
   "source": [
    "def refresh_bpe_vocab_by_merging_bigram(bigram: Tuple[str, str], old_bpe_vocab: Dict[str, int]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    在\"单词分词状态->频数\"的词典中，合并指定的bigram（即去掉对应的相邻unigram之间的空格），最后返回新的词典，例如：  \n",
    "    输入 \n",
    "    ```python\n",
    "    bigram = ('i', '</w>'), old_bpe_vocab = {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "    得到\n",
    "    ```python\n",
    "    {\n",
    "        'i</w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "    \n",
    "    Args:\n",
    "        bigram: Tuple[str, str] - 待合并的bigram\n",
    "        old_bpe_vocab: Dict[str, int] - 初始\"单词分词状态->频数\"的词典\n",
    "\n",
    "    Return:\n",
    "        Dict[str, int] - 合并后的\"单词分词状态->频数\"的词典\n",
    "    \"\"\"\n",
    "\n",
    "    new_bpe_vocab = dict()\n",
    "    bigram_str = ' '.join(bigram)\n",
    "    bigram_merged = ''.join(bigram)\n",
    "    # TODO: 完成函数体【1分】\n",
    "    pass\n",
    "    for word,freq in old_bpe_vocab.items():\n",
    "        if bigram_str in word:\n",
    "            new_word = word.replace(bigram_str,bigram_merged)\n",
    "            new_bpe_vocab[new_word] = freq\n",
    "        else:\n",
    "            new_bpe_vocab[word] = old_bpe_vocab[word]\n",
    "    return new_bpe_vocab\n",
    "\n",
    "bigram = ('h', 'a')\n",
    "\n",
    "old_bpe_vocab = {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "\n",
    "print(refresh_bpe_vocab_by_merging_bigram(bigram, old_bpe_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "992438a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i</w>', 2),\n",
       " ('</w>', 5),\n",
       " ('ha', 2),\n",
       " ('pp', 2),\n",
       " ('N', 1),\n",
       " ('a', 2),\n",
       " ('e', 2),\n",
       " ('l', 1),\n",
       " ('m', 1),\n",
       " ('s', 1),\n",
       " ('v', 1),\n",
       " ('y', 1)]"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_bpe_tokens(bpe_vocab: Dict[str, int]) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    根据\"单词分词状态->频数\"的词典，返回所得到的BPE分词列表，并将该列表按照分词长度降序排序返回，例如：  \n",
    "    输入 \n",
    "    ```python\n",
    "    bpe_vocab = {\n",
    "        'i</w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'ha pp y </w>': 1,\n",
    "        'ha v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a pp l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "    得到\n",
    "    ```\n",
    "    [\n",
    "        ('i</w>', 2),\n",
    "        ('ha', 2),\n",
    "        ('pp', 2),\n",
    "        ('a', 2),\n",
    "        ('m', 1),\n",
    "        ('</w>', 5),\n",
    "        ('y', 1),\n",
    "        ('v', 1),\n",
    "        ('e', 2),\n",
    "        ('N', 1),\n",
    "        ('l', 1),\n",
    "        ('s', 1)\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        bpe_vocab: Dict[str, int] - \"单词分词状态->频数\"的词典\n",
    "\n",
    "    Return:\n",
    "        List[Tuple[str, int]] - BPE分词和对应频数组成的List\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: 完成函数体【2分】\n",
    "    pass\n",
    "    bpe_tokens = []\n",
    "    \n",
    "    from collections import defaultdict\n",
    "    token_counts = defaultdict(int)\n",
    "\n",
    "    for word in bpe_vocab:\n",
    "        tokens = word.split()  # 将单词分词状态按空格拆分为分词\n",
    "        for token in tokens:\n",
    "            token_counts[token] += bpe_vocab[word]  # 累加频数\n",
    "    \n",
    "    # 将结果转换为列表，并按照分词长度降序排序\n",
    "    bpe_tokens = sorted(token_counts.items(), key=lambda x: (-len(x[0]), x[0]))\n",
    "\n",
    "    return bpe_tokens\n",
    "\n",
    "bpe_vocab = {\n",
    "        'i</w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'ha pp y </w>': 1,\n",
    "        'ha v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a pp l e s </w>': 1\n",
    "    }\n",
    "\n",
    "get_bpe_tokens(bpe_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "3c56995e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "su per <unknown>\n",
      "sh an g hai </w>\n"
     ]
    }
   ],
   "source": [
    "def print_bpe_tokenize(word: str, bpe_tokens: List[Tuple[str, int]]):\n",
    "    \"\"\"\n",
    "    根据按长度降序的BPE分词列表，将所给单词进行BPE分词，最后打印结果。\n",
    "    \n",
    "    思想是，对于一个待BPE分词的单词，按照长度顺序从列表中寻找BPE分词进行子串匹配，  \n",
    "    若成功匹配，则对该子串左右的剩余部分递归地进行下一轮匹配，直到剩余部分长度为0，  \n",
    "    或者剩余部分无法匹配（该部分整体由`\"<unknown>\"`代替）\n",
    "    \n",
    "    例1：  \n",
    "    输入 `word = \"supermarket\"`, `bpe_tokens=[\n",
    "        (\"su\", 20),\n",
    "        (\"are\", 10),\n",
    "        (\"per\", 30),\n",
    "    ]`  \n",
    "    最终打印 `\"su per <unknown>\"`\n",
    "\n",
    "    例2：  \n",
    "    输入 `word = \"shanghai\"`, `bpe_tokens=[\n",
    "        (\"hai\", 1),\n",
    "        (\"sh\", 1),\n",
    "        (\"an\", 1),\n",
    "        (\"</w>\", 1),\n",
    "        (\"g\", 1)\n",
    "    ]`  \n",
    "    最终打印 `\"sh an g hai </w>\"`\n",
    "\n",
    "    Args:\n",
    "        word: str - 待分词的单词\n",
    "        bpe_tokens: List[Tuple(str, int)] - BPE分词和对应频数组成的列表\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: 请尝试使用递归函数定义该分词过程【2分】\n",
    "    def bpe_tokenize(sub_word: str) -> str:\n",
    "        # 如果子串为空，直接返回空字符串\n",
    "        if not sub_word:\n",
    "            return \"\"\n",
    "\n",
    "        # 遍历 BPE 分词，寻找第一个在 sub_word 中出现的 token\n",
    "        for token, _ in bpe_tokens:\n",
    "            pos = sub_word.find(token)\n",
    "            if pos != -1:\n",
    "                # 找到匹配的分词，递归处理左右部分\n",
    "                left_part = sub_word[:pos]\n",
    "                right_part = sub_word[pos + len(token):]\n",
    "                return (\n",
    "                    (bpe_tokenize(left_part) + \" \" if left_part else \"\")\n",
    "                    + token + \" \"\n",
    "                    + (bpe_tokenize(right_part) if right_part else \"\")\n",
    "                ).strip()\n",
    "\n",
    "        # 如果没有找到匹配的分词，返回 \"<unknown>\"\n",
    "        return \"<unknown> \"\n",
    "\n",
    "    res = bpe_tokenize(word + \"</w>\")\n",
    "    print(res)\n",
    "\n",
    "# 测试示例\n",
    "bpe_tokens = [\n",
    "    (\"su\", 20),\n",
    "    (\"are\", 10),\n",
    "    (\"per\", 30),\n",
    "]\n",
    "print_bpe_tokenize(\"supermarket\", bpe_tokens)\n",
    "\n",
    "bpe_tokens=[\n",
    "        (\"hai\", 1),\n",
    "        (\"sh\", 1),\n",
    "        (\"an\", 1),\n",
    "        (\"</w>\", 1),\n",
    "        (\"g\", 1)\n",
    "    ]\n",
    "print_bpe_tokenize(\"shanghai\", bpe_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd70402",
   "metadata": {},
   "source": [
    "开始读取数据集并训练BPE分词器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "215b56d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training corpus.\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/news.2007.en.shuffled.deduped.train\", encoding=\"utf-8\") as f:\n",
    "    training_corpus = list(map(lambda l: l.strip(), f.readlines()[:1000]))\n",
    "\n",
    "print(\"Loaded training corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "7bccd41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Merged bigram ('e', '</w>')\n",
      "Iteration 1: Merged bigram ('s', '</w>')\n",
      "Iteration 2: Merged bigram ('t', 'h')\n",
      "Iteration 3: Merged bigram ('d', '</w>')\n",
      "Iteration 4: Merged bigram ('i', 'n')\n",
      "Iteration 5: Merged bigram ('t', '</w>')\n",
      "Iteration 6: Merged bigram ('e', 'r')\n",
      "Iteration 7: Merged bigram ('a', 'n')\n",
      "Iteration 8: Merged bigram ('o', 'n')\n",
      "Iteration 9: Merged bigram ('th', 'e</w>')\n",
      "Iteration 10: Merged bigram ('y', '</w>')\n",
      "Iteration 11: Merged bigram ('e', 'n')\n",
      "Iteration 12: Merged bigram ('o', 'r')\n",
      "Iteration 13: Merged bigram ('a', 'r')\n",
      "Iteration 14: Merged bigram ('o', '</w>')\n",
      "Iteration 15: Merged bigram ('e', 'd</w>')\n",
      "Iteration 16: Merged bigram ('a', 'l')\n",
      "Iteration 17: Merged bigram ('N', '</w>')\n",
      "Iteration 18: Merged bigram ('g', '</w>')\n",
      "Iteration 19: Merged bigram ('a', '</w>')\n",
      "Iteration 20: Merged bigram ('r', 'e')\n",
      "Iteration 21: Merged bigram ('o', 'u')\n",
      "Iteration 22: Merged bigram ('in', 'g</w>')\n",
      "Iteration 23: Merged bigram ('t', 'i')\n",
      "Iteration 24: Merged bigram ('f', '</w>')\n",
      "Iteration 25: Merged bigram ('er', '</w>')\n",
      "Iteration 26: Merged bigram ('on', '</w>')\n",
      "Iteration 27: Merged bigram ('t', 'o</w>')\n",
      "Iteration 28: Merged bigram ('an', 'd</w>')\n",
      "Iteration 29: Merged bigram ('o', 'f</w>')\n",
      "Iteration 30: Merged bigram ('s', 't')\n",
      "Iteration 31: Merged bigram ('in', '</w>')\n",
      "Iteration 32: Merged bigram ('o', 'm')\n",
      "Iteration 33: Merged bigram ('e', 's</w>')\n",
      "Iteration 34: Merged bigram ('i', 'c')\n",
      "Iteration 35: Merged bigram ('i', 'l')\n",
      "Iteration 36: Merged bigram ('c', 'h')\n",
      "Iteration 37: Merged bigram ('a', 't')\n",
      "Iteration 38: Merged bigram ('i', 's</w>')\n",
      "Iteration 39: Merged bigram ('a', 's</w>')\n",
      "Iteration 40: Merged bigram ('an', '</w>')\n",
      "Iteration 41: Merged bigram ('r', 'o')\n",
      "Iteration 42: Merged bigram ('e', 'l')\n",
      "Iteration 43: Merged bigram ('or', '</w>')\n",
      "Iteration 44: Merged bigram ('al', '</w>')\n",
      "Iteration 45: Merged bigram ('i', 't')\n",
      "Iteration 46: Merged bigram ('a', 'm')\n",
      "Iteration 47: Merged bigram ('s', 'h')\n",
      "Iteration 48: Merged bigram ('a', 'c')\n",
      "Iteration 49: Merged bigram ('l', '</w>')\n",
      "Iteration 50: Merged bigram ('a', 'i')\n",
      "Iteration 51: Merged bigram ('e', 'c')\n",
      "Iteration 52: Merged bigram ('en', '</w>')\n",
      "Iteration 53: Merged bigram ('w', 'h')\n",
      "Iteration 54: Merged bigram ('r', 'i')\n",
      "Iteration 55: Merged bigram ('e', 'm')\n",
      "Iteration 56: Merged bigram ('a', 's')\n",
      "Iteration 57: Merged bigram ('en', 't</w>')\n",
      "Iteration 58: Merged bigram ('o', 'l')\n",
      "Iteration 59: Merged bigram ('k', '</w>')\n",
      "Iteration 60: Merged bigram ('u', 'n')\n",
      "Iteration 61: Merged bigram ('l', 'i')\n",
      "Iteration 62: Merged bigram ('s', 'i')\n",
      "Iteration 63: Merged bigram ('n', 'e')\n",
      "Iteration 64: Merged bigram ('v', 'e</w>')\n",
      "Iteration 65: Merged bigram ('w', '</w>')\n",
      "Iteration 66: Merged bigram ('f', 'or</w>')\n",
      "Iteration 67: Merged bigram ('g', 'h')\n",
      "Iteration 68: Merged bigram ('p', 'l')\n",
      "Iteration 69: Merged bigram ('s', 'u')\n",
      "Iteration 70: Merged bigram ('th', 'at</w>')\n",
      "Iteration 71: Merged bigram ('r', '</w>')\n",
      "Iteration 72: Merged bigram ('t', 's</w>')\n",
      "Iteration 73: Merged bigram ('a', 'd')\n",
      "Iteration 74: Merged bigram (\"'\", 's</w>')\n",
      "Iteration 75: Merged bigram ('d', 'e')\n",
      "Iteration 76: Merged bigram ('a', 'y</w>')\n",
      "Iteration 77: Merged bigram ('l', 'e')\n",
      "Iteration 78: Merged bigram ('c', 'om')\n",
      "Iteration 79: Merged bigram ('l', 'o')\n",
      "Iteration 80: Merged bigram ('d', 'i')\n",
      "Iteration 81: Merged bigram ('a', 'g')\n",
      "Iteration 82: Merged bigram ('c', 'on')\n",
      "Iteration 83: Merged bigram ('b', 'u')\n",
      "Iteration 84: Merged bigram ('l', 'y</w>')\n",
      "Iteration 85: Merged bigram ('w', 'ith')\n",
      "Iteration 86: Merged bigram ('s', 'e</w>')\n",
      "Iteration 87: Merged bigram ('ai', 'd</w>')\n",
      "Iteration 88: Merged bigram ('s', 'e')\n",
      "Iteration 89: Merged bigram ('p', 'ro')\n",
      "Iteration 90: Merged bigram ('a', 'b')\n",
      "Iteration 91: Merged bigram ('n', '</w>')\n",
      "Iteration 92: Merged bigram ('with', '</w>')\n",
      "Iteration 93: Merged bigram ('th', 'e')\n",
      "Iteration 94: Merged bigram ('p', '</w>')\n",
      "Iteration 95: Merged bigram ('s', 'aid</w>')\n",
      "Iteration 96: Merged bigram ('b', 'e')\n",
      "Iteration 97: Merged bigram ('n', 'o')\n",
      "Iteration 98: Merged bigram ('s', 'p')\n",
      "Iteration 99: Merged bigram ('v', 'i')\n",
      "Iteration 100: Merged bigram ('u', 'r')\n",
      "Iteration 101: Merged bigram ('c', 'e</w>')\n",
      "Iteration 102: Merged bigram ('a', 'p')\n",
      "Iteration 103: Merged bigram ('h', 'e</w>')\n",
      "Iteration 104: Merged bigram ('i', '</w>')\n",
      "Iteration 105: Merged bigram ('v', 'er')\n",
      "Iteration 106: Merged bigram ('e', 'x')\n",
      "Iteration 107: Merged bigram ('w', 'as</w>')\n",
      "Iteration 108: Merged bigram ('h', 'a')\n",
      "Iteration 109: Merged bigram ('t', 'r')\n",
      "Iteration 110: Merged bigram ('f', 'f')\n",
      "Iteration 111: Merged bigram ('s', 's</w>')\n",
      "Iteration 112: Merged bigram ('a', 'k')\n",
      "Iteration 113: Merged bigram ('f', 'i')\n",
      "Iteration 114: Merged bigram ('ati', 'on</w>')\n",
      "Iteration 115: Merged bigram ('er', 's</w>')\n",
      "Iteration 116: Merged bigram ('b', 'y</w>')\n",
      "Iteration 117: Merged bigram ('p', 'e')\n",
      "Iteration 118: Merged bigram ('p', 'o')\n",
      "Iteration 119: Merged bigram ('t', 'ed</w>')\n",
      "Iteration 120: Merged bigram ('t', 'o')\n",
      "Iteration 121: Merged bigram ('d', 's</w>')\n",
      "Iteration 122: Merged bigram ('h', 'is</w>')\n",
      "Iteration 123: Merged bigram ('f', 'rom')\n",
      "Iteration 124: Merged bigram ('from', '</w>')\n",
      "Iteration 125: Merged bigram ('l', 'd</w>')\n",
      "Iteration 126: Merged bigram ('m', '</w>')\n",
      "Iteration 127: Merged bigram ('m', 'o')\n",
      "Iteration 128: Merged bigram ('at', 'e</w>')\n",
      "Iteration 129: Merged bigram ('d', 'ay</w>')\n",
      "Iteration 130: Merged bigram ('t', 'er</w>')\n",
      "Iteration 131: Merged bigram ('th', '</w>')\n",
      "Iteration 132: Merged bigram ('f', 'or')\n",
      "Iteration 133: Merged bigram ('a', 'u')\n",
      "Iteration 134: Merged bigram ('ha', 've</w>')\n",
      "Iteration 135: Merged bigram ('w', 'e')\n",
      "Iteration 136: Merged bigram ('ou', 't</w>')\n",
      "Iteration 137: Merged bigram ('y', 'e')\n",
      "Iteration 138: Merged bigram ('q', 'u')\n",
      "Iteration 139: Merged bigram ('on', 's</w>')\n",
      "Iteration 140: Merged bigram ('ou', 'ld</w>')\n",
      "Iteration 141: Merged bigram ('w', 'or')\n",
      "Iteration 142: Merged bigram ('il', 'l</w>')\n",
      "Iteration 143: Merged bigram ('h', 'i')\n",
      "Iteration 144: Merged bigram ('s', 'c')\n",
      "Iteration 145: Merged bigram ('a', 'f')\n",
      "Iteration 146: Merged bigram ('the', 'y</w>')\n",
      "Iteration 147: Merged bigram ('gh', 't</w>')\n",
      "Iteration 148: Merged bigram ('m', 'ent</w>')\n",
      "Iteration 149: Merged bigram ('f', 'e')\n",
      "Iteration 150: Merged bigram ('p', 're')\n",
      "Iteration 151: Merged bigram ('ne', 'w</w>')\n",
      "Iteration 152: Merged bigram ('bu', 't</w>')\n",
      "Iteration 153: Merged bigram ('w', 'ill</w>')\n",
      "Iteration 154: Merged bigram ('d', 'u')\n",
      "Iteration 155: Merged bigram ('t', 'er')\n",
      "Iteration 156: Merged bigram ('s', 'o</w>')\n",
      "Iteration 157: Merged bigram ('g', 'e')\n",
      "Iteration 158: Merged bigram ('m', 'e')\n",
      "Iteration 159: Merged bigram ('p', 'ar')\n",
      "Iteration 160: Merged bigram ('b', 'o')\n",
      "Iteration 161: Merged bigram ('p', 'u')\n",
      "Iteration 162: Merged bigram ('al', 'l</w>')\n",
      "Iteration 163: Merged bigram ('ti', 'on</w>')\n",
      "Iteration 164: Merged bigram ('a', 'y')\n",
      "Iteration 165: Merged bigram ('d', 'o')\n",
      "Iteration 166: Merged bigram ('t', 'e')\n",
      "Iteration 167: Merged bigram ('g', 'o')\n",
      "Iteration 168: Merged bigram ('m', 'i')\n",
      "Iteration 169: Merged bigram ('r', 'u')\n",
      "Iteration 170: Merged bigram ('k', 'e')\n",
      "Iteration 171: Merged bigram ('h', 'e')\n",
      "Iteration 172: Merged bigram ('i', 'r</w>')\n",
      "Iteration 173: Merged bigram ('c', 'u')\n",
      "Iteration 174: Merged bigram ('g', 'i')\n",
      "Iteration 175: Merged bigram ('ch', '</w>')\n",
      "Iteration 176: Merged bigram ('wh', 'o</w>')\n",
      "Iteration 177: Merged bigram ('m', 'ar')\n",
      "Iteration 178: Merged bigram ('no', 't</w>')\n",
      "Iteration 179: Merged bigram ('the', 'ir</w>')\n",
      "Iteration 180: Merged bigram ('c', 'l')\n",
      "Iteration 181: Merged bigram ('j', 'u')\n",
      "Iteration 182: Merged bigram ('gh', '</w>')\n",
      "Iteration 183: Merged bigram ('h', 'o')\n",
      "Iteration 184: Merged bigram ('m', 'an')\n",
      "Iteration 185: Merged bigram ('w', 'n</w>')\n",
      "Iteration 186: Merged bigram ('g', 'u')\n",
      "Iteration 187: Merged bigram ('y', 'ou')\n",
      "Iteration 188: Merged bigram ('c', 'oun')\n",
      "Iteration 189: Merged bigram ('w', 'ould</w>')\n",
      "Iteration 190: Merged bigram ('you', '</w>')\n",
      "Iteration 191: Merged bigram ('c', 'o')\n",
      "Iteration 192: Merged bigram ('c', 'i')\n",
      "Iteration 193: Merged bigram ('pe', 'o')\n",
      "Iteration 194: Merged bigram ('v', 'e')\n",
      "Iteration 195: Merged bigram ('peo', 'ple</w>')\n",
      "Iteration 196: Merged bigram (\"'\", 't</w>')\n",
      "Iteration 197: Merged bigram ('t', 'y</w>')\n",
      "Iteration 198: Merged bigram ('s', 'o')\n",
      "Iteration 199: Merged bigram ('ye', 'ar</w>')\n",
      "Iteration 200: Merged bigram ('g', 'r')\n",
      "Iteration 201: Merged bigram ('af', 'ter</w>')\n",
      "Iteration 202: Merged bigram ('al', 'so</w>')\n",
      "Iteration 203: Merged bigram ('on', 'al</w>')\n",
      "Iteration 204: Merged bigram ('m', 'u')\n",
      "Iteration 205: Merged bigram ('ar', 's</w>')\n",
      "Iteration 206: Merged bigram ('n', 'i')\n",
      "Iteration 207: Merged bigram ('al', 'ly</w>')\n",
      "Iteration 208: Merged bigram ('c', 'e')\n",
      "Iteration 209: Merged bigram ('k', 's</w>')\n",
      "Iteration 210: Merged bigram ('f', 'u')\n",
      "Iteration 211: Merged bigram ('ar', 'd</w>')\n",
      "Iteration 212: Merged bigram ('fi', 'r')\n",
      "Iteration 213: Merged bigram ('t', 'w')\n",
      "Iteration 214: Merged bigram ('ab', 'le</w>')\n",
      "Iteration 215: Merged bigram ('i', 'r')\n",
      "Iteration 216: Merged bigram ('i', 'm')\n",
      "Iteration 217: Merged bigram ('c', 'ar')\n",
      "Iteration 218: Merged bigram ('o', 'ther</w>')\n",
      "Iteration 219: Merged bigram ('be', 'en</w>')\n",
      "Iteration 220: Merged bigram ('c', 're')\n",
      "Iteration 221: Merged bigram ('an', 't</w>')\n",
      "Iteration 222: Merged bigram ('sh', '</w>')\n",
      "Iteration 223: Merged bigram ('si', 'on</w>')\n",
      "Iteration 224: Merged bigram ('in', 'to</w>')\n",
      "Iteration 225: Merged bigram ('ar', 'y</w>')\n",
      "Iteration 226: Merged bigram ('r', 'an')\n",
      "Iteration 227: Merged bigram ('c', 'an</w>')\n",
      "Iteration 228: Merged bigram ('w', 'ay</w>')\n",
      "Iteration 229: Merged bigram ('k', 'no')\n",
      "Iteration 230: Merged bigram ('j', 'o')\n",
      "Iteration 231: Merged bigram ('k', 'ing</w>')\n",
      "Iteration 232: Merged bigram ('ac', 'k</w>')\n",
      "Iteration 233: Merged bigram ('th', 're')\n",
      "Iteration 234: Merged bigram ('li', 'on</w>')\n",
      "Iteration 235: Merged bigram ('a', 'v')\n",
      "Iteration 236: Merged bigram ('x', '</w>')\n",
      "Iteration 237: Merged bigram ('tw', 'o</w>')\n",
      "Iteration 238: Merged bigram ('ay', 's</w>')\n",
      "Iteration 239: Merged bigram ('ati', 'ons</w>')\n",
      "Iteration 240: Merged bigram ('u', '</w>')\n",
      "Iteration 241: Merged bigram ('t', 'ur')\n",
      "Iteration 242: Merged bigram ('g', 's</w>')\n",
      "Iteration 243: Merged bigram ('cl', 'u')\n",
      "Iteration 244: Merged bigram ('b', '</w>')\n",
      "Iteration 245: Merged bigram ('u', 's</w>')\n",
      "Iteration 246: Merged bigram ('w', 's</w>')\n",
      "Iteration 247: Merged bigram ('e', 'ar')\n",
      "Iteration 248: Merged bigram ('i', 'f</w>')\n",
      "Iteration 249: Merged bigram ('l', 's</w>')\n",
      "Iteration 250: Merged bigram ('o', 'ver</w>')\n",
      "Iteration 251: Merged bigram ('in', 'ter')\n",
      "Iteration 252: Merged bigram ('por', 't</w>')\n",
      "Iteration 253: Merged bigram ('s', 'day</w>')\n",
      "Iteration 254: Merged bigram ('s', 'y')\n",
      "Iteration 255: Merged bigram ('ver', 'y</w>')\n",
      "Iteration 256: Merged bigram ('an', 'y</w>')\n",
      "Iteration 257: Merged bigram ('p', 's</w>')\n",
      "Iteration 258: Merged bigram ('h', 'u')\n",
      "Iteration 259: Merged bigram ('mo', 'st</w>')\n",
      "Iteration 260: Merged bigram ('g', 'n</w>')\n",
      "Iteration 261: Merged bigram ('an', 's</w>')\n",
      "Iteration 262: Merged bigram ('m', 'r</w>')\n",
      "Iteration 263: Merged bigram ('n', 'ati')\n",
      "Iteration 264: Merged bigram ('fir', 'st</w>')\n",
      "Iteration 265: Merged bigram ('c', 'an')\n",
      "Iteration 266: Merged bigram ('i', 's')\n",
      "Iteration 267: Merged bigram ('u', 'p</w>')\n",
      "Iteration 268: Merged bigram ('ge', 't</w>')\n",
      "Iteration 269: Merged bigram ('mil', 'lion</w>')\n",
      "Iteration 270: Merged bigram ('j', 'ec')\n",
      "Iteration 271: Merged bigram ('g', 'y</w>')\n",
      "Iteration 272: Merged bigram ('t', 'ly</w>')\n",
      "Iteration 273: Merged bigram ('du', 'c')\n",
      "Iteration 274: Merged bigram ('t', 'u')\n",
      "Iteration 275: Merged bigram ('n', 'u')\n",
      "Iteration 276: Merged bigram ('e', 't</w>')\n",
      "Iteration 277: Merged bigram ('l', 'an')\n",
      "Iteration 278: Merged bigram ('nati', 'onal</w>')\n",
      "Iteration 279: Merged bigram ('d', 'y</w>')\n",
      "Iteration 280: Merged bigram ('f', 'ri')\n",
      "Iteration 281: Merged bigram ('thre', 'e</w>')\n",
      "Iteration 282: Merged bigram ('li', 'ke</w>')\n",
      "Iteration 283: Merged bigram ('pre', 'si')\n",
      "Iteration 284: Merged bigram ('st', 'ar')\n",
      "Iteration 285: Merged bigram ('in', 've')\n",
      "Iteration 286: Merged bigram ('b', 'i')\n",
      "Iteration 287: Merged bigram ('in', 'clu')\n",
      "Iteration 288: Merged bigram ('ag', 'ain')\n",
      "Iteration 289: Merged bigram ('again', 'st</w>')\n",
      "Iteration 290: Merged bigram ('on', 'ly</w>')\n",
      "Iteration 291: Merged bigram ('presi', 'dent</w>')\n",
      "Iteration 292: Merged bigram ('per', 'cent</w>')\n",
      "Iteration 293: Merged bigram ('v', 'o')\n",
      "Iteration 294: Merged bigram ('b', 'ack</w>')\n",
      "Iteration 295: Merged bigram ('bu', 'sine')\n",
      "Iteration 296: Merged bigram ('c', '</w>')\n",
      "Iteration 297: Merged bigram ('c', 'al')\n",
      "Iteration 298: Merged bigram ('o', 'd</w>')\n",
      "Iteration 299: Merged bigram ('au', 'se</w>')\n",
      "Final BPE tokens: [('demolishing</w>', 1), ('generically</w>', 1), ('economists</w>', 1), ('monitoring</w>', 1), ('soliciting</w>', 1), ('agination</w>', 2), ('americans</w>', 5), ('asurement</w>', 1), ('economist</w>', 1), ('ministers</w>', 4), ('political</w>', 6), ('president</w>', 23), ('prominent</w>', 1), ('punishing</w>', 1), ('stination</w>', 1), ('africans</w>', 1), ('american</w>', 11), ('asteland</w>', 1), ('becoming</w>', 1), ('clinical</w>', 1), ('decision</w>', 3), ('declined</w>', 1), ('dination</w>', 1), ('discover</w>', 1), ('donation</w>', 1), ('economic</w>', 6), ('finished</w>', 2), ('historic</w>', 2), ('homeland</w>', 1), ('honoring</w>', 2), ('maritime</w>', 1), ('michelin</w>', 1), ('minister</w>', 16), ('national</w>', 25), ('parisons</w>', 1), ('promised</w>', 1), ('risoners</w>', 1), ('scheming</w>', 1), ('security</w>', 6), ('sination</w>', 1), ('sorority</w>', 1), ('timeline</w>', 1), ('tirement</w>', 1), ('tisement</w>', 1), ('tremists</w>', 1), ('trically</w>', 1), ('verished</w>', 1), ('visiting</w>', 1), ('ability</w>', 5), ('acement</w>', 1), ('achable</w>', 1), ('adition</w>', 1), ('african</w>', 2), ('against</w>', 23), ('agement</w>', 9), ('aisical</w>', 1), ('alition</w>', 2), ('another</w>', 13), ('ashamed</w>', 1), ('asoline</w>', 1), ('astrous</w>', 1), ('atement</w>', 10), ('atholic</w>', 1), ('aturely</w>', 1), ('aturing</w>', 1), ('aveling</w>', 1), ('avorite</w>', 2), ('becomes</w>', 2), ('caloric</w>', 1), ('chasing</w>', 2), ('decline</w>', 2), ('delines</w>', 1), ('dinitis</w>', 1), ('element</w>', 1), ('fearing</w>', 1), ('fferent</w>', 5), ('ffering</w>', 2), ('fficers</w>', 5), ('fically</w>', 2), ('fishing</w>', 3), ('generes</w>', 1), ('hadists</w>', 1), ('hearing</w>', 2), ('homered</w>', 1), ('honored</w>', 2), ('irement</w>', 1), ('isition</w>', 2), ('jorisch</w>', 1), ('limited</w>', 2), ('listing</w>', 1), ('marines</w>', 1), ('million</w>', 26), ('nations</w>', 1), ('nically</w>', 1), ('parison</w>', 1), ('pearing</w>', 1), ('pelican</w>', 1), ('percent</w>', 23), ('plished</w>', 1), ('promise</w>', 3), ('recover</w>', 1), ('richard</w>', 2), ('ritical</w>', 1), ('ronicle</w>', 1), ('scoring</w>', 2), ('secured</w>', 3), ('shaking</w>', 1), ('sharing</w>', 1), ('sically</w>', 1), ('sistent</w>', 2), ('sisting</w>', 1), ('speople</w>', 1), ('stomers</w>', 5), ('tenured</w>', 1), ('thening</w>', 1), ('thering</w>', 1), ('tourist</w>', 1), ('tremely</w>', 2), ('uniting</w>', 1), ('verines</w>', 1), ('visited</w>', 1), ('wearing</w>', 2), ('without</w>', 4), ('achers</w>', 1), ('aching</w>', 2), ('adited</w>', 1), ('affiti</w>', 1), ('ailers</w>', 2), ('ailing</w>', 3), ('aining</w>', 13), ('airing</w>', 1), ('aisers</w>', 1), ('aising</w>', 2), ('aiting</w>', 4), ('akened</w>', 1), ('alised</w>', 1), ('alises</w>', 1), ('alists</w>', 3), ('aminer</w>', 1), ('anised</w>', 3), ('apolis</w>', 2), ('ascent</w>', 1), ('asered</w>', 1), ('ashing</w>', 2), ('asites</w>', 1), ('asoned</w>', 1), ('asting</w>', 3), ('asures</w>', 4), ('atened</w>', 2), ('athing</w>', 1), ('ations</w>', 32), ('atitis</w>', 1), ('atured</w>', 2), ('atures</w>', 3), ('aurant</w>', 2), ('become</w>', 5), ('bicles</w>', 1), ('bilisi</w>', 1), ('bility</w>', 2), ('bining</w>', 1), ('boring</w>', 1), ('cecily</w>', 1), ('cement</w>', 5), ('chased</w>', 1), ('citing</w>', 1), ('clinic</w>', 2), ('colour</w>', 1), ('coming</w>', 3), ('decent</w>', 2), ('deline</w>', 1), ('demand</w>', 3), ('dening</w>', 1), ('dering</w>', 1), ('dicine</w>', 2), ('dining</w>', 1), ('dishes</w>', 1), ('dition</w>', 7), ('ducers</w>', 1), ('ducing</w>', 2), ('during</w>', 14), ('ffered</w>', 4), ('fficer</w>', 4), ('ffices</w>', 2), ('ficant</w>', 3), ('filing</w>', 1), ('finity</w>', 1), ('firing</w>', 1), ('fiscal</w>', 2), ('gement</w>', 1), ('gering</w>', 1), ('gerous</w>', 1), ('gilant</w>', 2), ('habits</w>', 1), ('hailed</w>', 1), ('haring</w>', 1), ('hatred</w>', 1), ('having</w>', 6), ('helich</w>', 1), ('hicles</w>', 3), ('hiring</w>', 1), ('homers</w>', 1), ('jority</w>', 1), ('lecito</w>', 1), ('lement</w>', 4), ('licans</w>', 2), ('licity</w>', 3), ('limits</w>', 1), ('lining</w>', 1), ('lished</w>', 2), ('lishes</w>', 2), ('listed</w>', 2), ('listic</w>', 2), ('lorist</w>', 1), ('marine</w>', 1), ('merely</w>', 1), ('merous</w>', 4), ('michel</w>', 1), ('mining</w>', 2), ('morist</w>', 1), ('mother</w>', 4), ('munist</w>', 2), ('munity</w>', 2), ('nation</w>', 2), ('nicole</w>', 3), ('nished</w>', 2), ('niture</w>', 1), ('parent</w>', 2), ('peared</w>', 6), ('pening</w>', 2), ('people</w>', 46), ('planes</w>', 2), ('police</w>', 15), ('ponent</w>', 1), ('reared</w>', 1), ('recall</w>', 3), ('recent</w>', 5), ('ricane</w>', 1), ('rimary</w>', 1), ('rising</w>', 4), ('risons</w>', 1), ('riting</w>', 1), ('ritish</w>', 5), ('rorist</w>', 1), ('scaled</w>', 1), ('scenes</w>', 1), ('scored</w>', 4), ('scores</w>', 2), ('secret</w>', 2), ('secure</w>', 1), ('seling</w>', 2), ('sement</w>', 1), ('shared</w>', 2), ('shares</w>', 9), ('sharif</w>', 1), ('should</w>', 15), ('sisted</w>', 1), ('sister</w>', 2), ('sition</w>', 8), ('splant</w>', 1), ('stical</w>', 1), ('stomer</w>', 1), ('stores</w>', 3), ('strand</w>', 1), ('string</w>', 1), ('surers</w>', 1), ('suring</w>', 2), ('tering</w>', 2), ('thored</w>', 1), ('ticals</w>', 1), ('timely</w>', 1), ('timing</w>', 2), ('timore</w>', 2), ('tition</w>', 2), ('trical</w>', 1), ('turers</w>', 3), ('turing</w>', 3), ('united</w>', 16), ('veland</w>', 1), ('vement</w>', 7), ('verano</w>', 1), ('verely</w>', 1), ('vering</w>', 3), ('vising</w>', 1), ('vision</w>', 10), ('visits</w>', 1), ('wering</w>', 1), ('within</w>', 5), ('younis</w>', 1), ('abour</w>', 3), ('about</w>', 41), ('ached</w>', 3), ('acher</w>', 1), ('aches</w>', 3), ('acing</w>', 3), ('acity</w>', 2), ('acles</w>', 1), ('aders</w>', 7), ('ading</w>', 12), ('afers</w>', 1), ('affer</w>', 1), ('affic</w>', 4), ('after</w>', 44), ('again</w>', 7), ('agels</w>', 1), ('agers</w>', 5), ('agine</w>', 1), ('aging</w>', 2), ('ailed</w>', 7), ('ailer</w>', 3), ('aimed</w>', 2), ('ained</w>', 17), ('aires</w>', 1), ('aised</w>', 6), ('aiser</w>', 1), ('aises</w>', 1), ('akers</w>', 6), ('aking</w>', 17), ('alant</w>', 1), ('alent</w>', 3), ('alice</w>', 1), ('alili</w>', 1), ('aling</w>', 3), ('alino</w>', 1), ('alist</w>', 4), ('ality</w>', 7), ('ament</w>', 6), ('amers</w>', 1), ('amily</w>', 10), ('amine</w>', 3), ('aming</w>', 3), ('amous</w>', 4), ('aning</w>', 2), ('anish</w>', 2), ('apers</w>', 4), ('apled</w>', 1), ('apons</w>', 1), ('arily</w>', 3), ('aring</w>', 2), ('ascal</w>', 1), ('ascus</w>', 1), ('ashir</w>', 1), ('asily</w>', 4), ('asing</w>', 3), ('asite</w>', 2), ('asons</w>', 3), ('aster</w>', 3), ('astic</w>', 3), ('astly</w>', 1), ('aston</w>', 1), ('astro</w>', 1), ('asure</w>', 2), ('ately</w>', 4), ('athan</w>', 2), ('ather</w>', 9), ('ating</w>', 24), ('ation</w>', 100), ('atric</w>', 1), ('ature</w>', 5), ('avily</w>', 1), ('aving</w>', 5), ('avish</w>', 1), ('ayers</w>', 6), ('bears</w>', 1), ('berus</w>', 2), ('bined</w>', 2), ('bited</w>', 2), ('bolic</w>', 1), ('bonus</w>', 1), ('calif</w>', 2), ('chain</w>', 1), ('chair</w>', 1), ('chard</w>', 1), ('chase</w>', 2), ('chemo</w>', 1), ('chers</w>', 1), ('child</w>', 8), ('chile</w>', 1), ('chill</w>', 2), ('ching</w>', 9), ('cited</w>', 3), ('clear</w>', 13), ('cling</w>', 1), ('comed</w>', 1), ('comes</w>', 4), ('could</w>', 19), ('cover</w>', 2), ('cured</w>', 1), ('decor</w>', 1), ('dened</w>', 2), ('dered</w>', 8), ('dical</w>', 8), ('dimes</w>', 1), ('dines</w>', 1), ('dited</w>', 4), ('ditor</w>', 3), ('dolan</w>', 1), ('domed</w>', 1), ('doned</w>', 1), ('donor</w>', 1), ('duced</w>', 4), ('ducer</w>', 3), ('duces</w>', 1), ('duran</w>', 2), ('dures</w>', 1), ('eling</w>', 3), ('elite</w>', 2), ('ement</w>', 4), ('eming</w>', 1), ('ening</w>', 1), ('ering</w>', 5), ('erome</w>', 1), ('fears</w>', 1), ('ffers</w>', 1), ('ffice</w>', 10), ('ffing</w>', 1), ('ffith</w>', 1), ('ficit</w>', 2), ('filed</w>', 2), ('fined</w>', 1), ('fired</w>', 2), ('first</w>', 28), ('furan</w>', 1), ('geles</w>', 2), ('genes</w>', 1), ('gered</w>', 3), ('ghold</w>', 1), ('ghout</w>', 6), ('gical</w>', 6), ('gines</w>', 1), ('gists</w>', 2), ('grand</w>', 2), ('grant</w>', 3), ('grity</w>', 1), ('group</w>', 12), ('gures</w>', 2), ('hagic</w>', 1), ('halis</w>', 1), ('hamed</w>', 2), ('hamit</w>', 1), ('hasis</w>', 1), ('haven</w>', 4), ('heard</w>', 3), ('helen</w>', 1), ('henin</w>', 1), ('hicle</w>', 2), ('hired</w>', 1), ('hiron</w>', 1), ('homer</w>', 1), ('homes</w>', 5), ('hones</w>', 1), ('horan</w>', 1), ('icher</w>', 1), ('imity</w>', 1), ('ining</w>', 2), ('inter</w>', 3), ('iring</w>', 2), ('ither</w>', 3), ('jones</w>', 1), ('jured</w>', 4), ('lican</w>', 1), ('licic</w>', 1), ('limit</w>', 1), ('lined</w>', 1), ('lines</w>', 2), ('milan</w>', 1), ('miles</w>', 5), ('mirel</w>', 1), ('mored</w>', 1), ('niman</w>', 1), ('nited</w>', 1), ('nomis</w>', 1), ('nored</w>', 1), ('olent</w>', 3), ('oning</w>', 4), ('ority</w>', 1), ('other</w>', 37), ('pared</w>', 9), ('pears</w>', 1), ('pened</w>', 3), ('pered</w>', 3), ('peres</w>', 1), ('plane</w>', 5), ('plans</w>', 6), ('plant</w>', 3), ('plene</w>', 1), ('pline</w>', 1), ('poles</w>', 1), ('preme</w>', 2), ('reary</w>', 1), ('relet</w>', 1), ('rices</w>', 5), ('rimes</w>', 1), ('riser</w>', 1), ('rises</w>', 1), ('risis</w>', 3), ('rison</w>', 1), ('riter</w>', 1), ('rites</w>', 1), ('rored</w>', 1), ('rover</w>', 1), ('scene</w>', 3), ('scent</w>', 1), ('scher</w>', 1), ('score</w>', 2), ('shall</w>', 1), ('shame</w>', 1), ('shape</w>', 1), ('share</w>', 7), ('shave</w>', 1), ('shene</w>', 1), ('shing</w>', 5), ('shire</w>', 2), ('shore</w>', 4), ('sical</w>', 2), ('sists</w>', 1), ('sites</w>', 2), ('sonal</w>', 2), ('sored</w>', 1), ('spent</w>', 3), ('stars</w>', 2), ('sters</w>', 4), ('stice</w>', 3), ('still</w>', 8), ('stine</w>', 1), ('sting</w>', 5), ('stion</w>', 4), ('stone</w>', 5), ('store</w>', 5), ('strom</w>', 1), ('sunil</w>', 1), ('sured</w>', 1), ('surer</w>', 1), ('sures</w>', 1), ('tened</w>', 2), ('tenor</w>', 1), ('teran</w>', 1), ('tered</w>', 2), ('their</w>', 56), ('theme</w>', 1), ('there</w>', 35), ('thers</w>', 8), ('thing</w>', 14), ('three</w>', 24), ('tical</w>', 3), ('ticed</w>', 1), ('tices</w>', 1), ('ticle</w>', 2), ('tiles</w>', 1), ('times</w>', 6), ('tired</w>', 1), ('tires</w>', 2), ('tists</w>', 2), ('tonin</w>', 1), ('toric</w>', 2), ('treme</w>', 1), ('trols</w>', 1), ('trout</w>', 1), ('truro</w>', 1), ('tured</w>', 2), ('tures</w>', 5), ('units</w>', 2), ('venir</w>', 1), ('vered</w>', 1), ('vices</w>', 14), ('virus</w>', 1), ('vised</w>', 1), ('visit</w>', 4), ('vists</w>', 2), ('wered</w>', 1), ('weren</w>', 1), ('where</w>', 19), ('which</w>', 49), ('while</w>', 24), ('white</w>', 9), ('whole</w>', 1), ('would</w>', 49), ('years</w>', 27), ('abel</w>', 2), ('able</w>', 39), ('abon</w>', 1), ('abor</w>', 2), ('aced</w>', 6), ('aces</w>', 4), ('ache</w>', 1), ('achi</w>', 1), ('acho</w>', 1), ('acle</w>', 3), ('acre</w>', 1), ('aded</w>', 4), ('aden</w>', 1), ('ader</w>', 8), ('ades</w>', 1), ('ador</w>', 2), ('adou</w>', 1), ('afer</w>', 1), ('aged</w>', 5), ('agen</w>', 1), ('ager</w>', 4), ('ages</w>', 9), ('agon</w>', 1), ('ails</w>', 4), ('aily</w>', 1), ('aire</w>', 1), ('aise</w>', 5), ('aisy</w>', 1), ('aith</w>', 1), ('aits</w>', 1), ('aken</w>', 7), ('aker</w>', 3), ('akes</w>', 11), ('aler</w>', 1), ('ales</w>', 9), ('alex</w>', 3), ('alin</w>', 1), ('ally</w>', 36), ('also</w>', 44), ('amed</w>', 8), ('amel</w>', 1), ('ames</w>', 10), ('amex</w>', 1), ('amic</w>', 2), ('amin</w>', 4), ('anel</w>', 3), ('aner</w>', 1), ('anic</w>', 4), ('anor</w>', 1), ('aper</w>', 6), ('aren</w>', 1), ('arim</w>', 1), ('aron</w>', 2), ('arun</w>', 1), ('ased</w>', 25), ('ases</w>', 16), ('asic</w>', 3), ('asif</w>', 1), ('asir</w>', 1), ('asis</w>', 2), ('ason</w>', 11), ('asts</w>', 1), ('ated</w>', 50), ('aten</w>', 1), ('ater</w>', 25), ('ates</w>', 42), ('atic</w>', 7), ('atin</w>', 3), ('ator</w>', 11), ('ause</w>', 23), ('avel</w>', 2), ('aven</w>', 1), ('aves</w>', 1), ('avor</w>', 2), ('ayed</w>', 5), ('ayer</w>', 3), ('back</w>', 23), ('bear</w>', 1), ('been</w>', 37), ('belichic', 1), ('bers</w>', 10), ('bile</w>', 3), ('bill</w>', 4), ('bine</w>', 1), ('bing</w>', 4), ('bone</w>', 1), ('call</w>', 4), ('cals</w>', 1), ('card</w>', 1), ('care</w>', 10), ('cars</w>', 2), ('celo</w>', 1), ('cely</w>', 1), ('cent</w>', 5), ('cers</w>', 1), ('chad</w>', 1), ('char</w>', 1), ('chat</w>', 1), ('ched</w>', 6), ('chen</w>', 2), ('cher</w>', 2), ('ches</w>', 5), ('cils</w>', 1), ('cing</w>', 9), ('city</w>', 12), ('cold</w>', 1), ('cole</w>', 1), ('come</w>', 19), ('cous</w>', 1), ('dels</w>', 1), ('dely</w>', 1), ('dent</w>', 14), ('ders</w>', 15), ('ding</w>', 70), ('dire</w>', 1), ('dise</w>', 1), ('dish</w>', 2), ('dits</w>', 1), ('done</w>', 4), ('duce</w>', 6), ('ears</w>', 1), ('eces</w>', 1), ('economic', 1), ('eled</w>', 1), ('eler</w>', 1), ('emed</w>', 6), ('emic</w>', 1), ('ener</w>', 1), ('ered</w>', 2), ('fear</w>', 5), ('fers</w>', 3), ('ffed</w>', 1), ('ffer</w>', 4), ('ffet</w>', 1), ('file</w>', 3), ('fine</w>', 2), ('fing</w>', 1), ('fire</w>', 6), ('fits</w>', 2), ('fore</w>', 12), ('from</w>', 99), ('gear</w>', 2), ('gely</w>', 5), ('gent</w>', 1), ('gers</w>', 8), ('gham</w>', 2), ('ghan</w>', 3), ('gher</w>', 6), ('gime</w>', 2), ('ging</w>', 4), ('gist</w>', 1), ('gold</w>', 2), ('gone</w>', 3), ('gore</w>', 1), ('grim</w>', 1), ('gure</w>', 2), ('habi</w>', 1), ('hail</w>', 1), ('hair</w>', 1), ('hall</w>', 1), ('halo</w>', 1), ('hand</w>', 3), ('hany</w>', 1), ('haps</w>', 3), ('hard</w>', 11), ('hats</w>', 1), ('have</w>', 92), ('held</w>', 8), ('here</w>', 12), ('hers</w>', 1), ('hill</w>', 1), ('hire</w>', 2), ('hist</w>', 1), ('hits</w>', 3), ('hold</w>', 4), ('hole</w>', 1), ('home</w>', 16), ('hone</w>', 6), ('hour</w>', 5), ('ical</w>', 1), ('iced</w>', 1), ('ices</w>', 1), ('icon</w>', 1), ('iled</w>', 1), ('iler</w>', 2), ('ilis</w>', 1), ('ined</w>', 3), ('ines</w>', 1), ('into</w>', 36), ('iran</w>', 5), ('ised</w>', 1), ('ited</w>', 1), ('item</w>', 1), ('ites</w>', 1), ('june</w>', 5), ('kely</w>', 8), ('kent</w>', 1), ('kers</w>', 8), ('king</w>', 34), ('knor</w>', 1), ('land</w>', 15), ('lent</w>', 1), ('lers</w>', 4), ('like</w>', 24), ('line</w>', 17), ('ling</w>', 36), ('lion</w>', 7), ('lish</w>', 3), ('list</w>', 4), ('lout</w>', 1), ('mand</w>', 3), ('many</w>', 14), ('mars</w>', 1), ('mary</w>', 2), ('mechanis', 1), ('memo</w>', 1), ('ment</w>', 72), ('mers</w>', 5), ('mich</w>', 1), ('mild</w>', 1), ('mile</w>', 1), ('mine</w>', 1), ('ming</w>', 5), ('mini</w>', 2), ('mino</w>', 1), ('miss</w>', 3), ('mith</w>', 1), ('mits</w>', 2), ('mons</w>', 2), ('more</w>', 50), ('most</w>', 29), ('mous</w>', 1), ('mune</w>', 1), ('near</w>', 5), ('nels</w>', 1), ('ners</w>', 5), ('nine</w>', 8), ('ning</w>', 20), ('none</w>', 1), ('oled</w>', 1), ('oman</w>', 10), ('omed</w>', 1), ('omen</w>', 14), ('omer</w>', 1), ('onal</w>', 19), ('oned</w>', 8), ('onen</w>', 1), ('oner</w>', 2), ('ones</w>', 2), ('only</w>', 23), ('over</w>', 30), ('pear</w>', 2), ('pers</w>', 5), ('plan</w>', 9), ('pler</w>', 1), ('ples</w>', 2), ('plex</w>', 3), ('pole</w>', 1), ('politici', 3), ('poly</w>', 1), ('port</w>', 30), ('prelimin', 1), ('prominen', 1), ('pure</w>', 2), ('rand</w>', 4), ('rear</w>', 2), ('rent</w>', 12), ('rice</w>', 10), ('rich</w>', 3), ('rill</w>', 1), ('rime</w>', 8), ('rine</w>', 1), ('ring</w>', 11), ('rini</w>', 1), ('rise</w>', 5), ('rist</w>', 2), ('rite</w>', 1), ('role</w>', 4), ('rome</w>', 1), ('runo</w>', 1), ('said</w>', 153), ('scle</w>', 1), ('sday</w>', 29), ('securiti', 2), ('sels</w>', 2), ('sent</w>', 6), ('sers</w>', 2), ('shed</w>', 4), ('shes</w>', 1), ('sile</w>', 2), ('sing</w>', 21), ('sion</w>', 23), ('sist</w>', 2), ('site</w>', 11), ('sity</w>', 9), ('sold</w>', 3), ('some</w>', 27), ('sons</w>', 1), ('sore</w>', 1), ('star</w>', 4), ('sted</w>', 8), ('stem</w>', 9), ('ster</w>', 5), ('stic</w>', 3), ('stin</w>', 1), ('stir</w>', 1), ('stly</w>', 1), ('ston</w>', 9), ('stor</w>', 2), ('sure</w>', 13), ('tear</w>', 1), ('tech</w>', 2), ('tele</w>', 1), ('tels</w>', 2), ('tely</w>', 5), ('tent</w>', 2), ('ters</w>', 16), ('than</w>', 32), ('that</w>', 198), ('them</w>', 28), ('then</w>', 16), ('ther</w>', 11), ('they</w>', 77), ('this</w>', 69), ('thor</w>', 1), ('thur</w>', 1), ('tice</w>', 4), ('time</w>', 31), ('tine</w>', 1), ('ting</w>', 75), ('tion</w>', 66), ('tire</w>', 2), ('tise</w>', 3), ('tish</w>', 2), ('tist</w>', 6), ('tite</w>', 1), ('tity</w>', 2), ('told</w>', 12), ('toni</w>', 1), ('tons</w>', 1), ('tour</w>', 2), ('tous</w>', 1), ('trol</w>', 2), ('tune</w>', 2), ('ture</w>', 16), ('unit</w>', 1), ('ures</w>', 1), ('vels</w>', 2), ('vely</w>', 7), ('vent</w>', 7), ('vere</w>', 1), ('vers</w>', 12), ('very</w>', 29), ('vice</w>', 19), ('vich</w>', 1), ('vili</w>', 1), ('vine</w>', 2), ('ving</w>', 28), ('vino</w>', 1), ('vist</w>', 2), ('vour</w>', 1), ('wear</w>', 2), ('wels</w>', 1), ('went</w>', 6), ('were</w>', 45), ('wers</w>', 3), ('what</w>', 16), ('when</w>', 47), ('whom</w>', 1), ('will</w>', 70), ('with</w>', 155), ('wore</w>', 1), ('year</w>', 45), ('yers</w>', 2), ('your</w>', 12), ('abiliti', 6), ('abu</w>', 2), ('aby</w>', 3), ('ace</w>', 35), ('ach</w>', 37), ('achicol', 2), ('aciliti', 1), ('ack</w>', 11), ('aco</w>', 1), ('ade</w>', 20), ('adi</w>', 3), ('adicali', 1), ('ado</w>', 1), ('ads</w>', 12), ('ady</w>', 9), ('afe</w>', 3), ('aff</w>', 3), ('age</w>', 46), ('ago</w>', 10), ('ags</w>', 1), ('aic</w>', 1), ('aid</w>', 5), ('ail</w>', 9), ('aim</w>', 3), ('ain</w>', 32), ('air</w>', 18), ('ait</w>', 1), ('ake</w>', 48), ('ald</w>', 1), ('ale</w>', 7), ('ali</w>', 3), ('all</w>', 59), ('alo</w>', 2), ('als</w>', 28), ('aly</w>', 3), ('amanati', 1), ('ame</w>', 44), ('ami</w>', 6), ('and</w>', 518), ('ane</w>', 1), ('ani</w>', 5), ('ano</w>', 3), ('ans</w>', 14), ('ant</w>', 21), ('anu</w>', 1), ('any</w>', 29), ('ape</w>', 2), ('aps</w>', 2), ('ard</w>', 36), ('are</w>', 87), ('ari</w>', 1), ('aro</w>', 1), ('ars</w>', 8), ('ary</w>', 35), ('ase</w>', 37), ('ash</w>', 10), ('asi</w>', 1), ('ass</w>', 12), ('ast</w>', 45), ('asy</w>', 4), ('ate</w>', 95), ('ath</w>', 11), ('ato</w>', 2), ('ats</w>', 7), ('aus</w>', 1), ('ave</w>', 12), ('ays</w>', 33), ('bec</w>', 1), ('bed</w>', 7), ('bel</w>', 1), ('ber</w>', 44), ('bes</w>', 1), ('bet</w>', 2), ('bic</w>', 1), ('bim</w>', 1), ('bol</w>', 1), ('bon</w>', 2), ('bor</w>', 1), ('bus</w>', 1), ('but</w>', 70), ('cal</w>', 10), ('can</w>', 35), ('car</w>', 6), ('carolin', 4), ('ced</w>', 15), ('cer</w>', 6), ('ceremon', 3), ('ces</w>', 14), ('che</w>', 1), ('cil</w>', 5), ('cle</w>', 3), ('clearan', 1), ('col</w>', 1), ('com</w>', 16), ('cor</w>', 3), ('cup</w>', 4), ('day</w>', 65), ('dec</w>', 1), ('ded</w>', 59), ('del</w>', 3), ('den</w>', 6), ('der</w>', 38), ('des</w>', 11), ('det</w>', 1), ('dex</w>', 4), ('dis</w>', 1), ('discoun', 2), ('distric', 8), ('dit</w>', 5), ('dol</w>', 1), ('dom</w>', 1), ('don</w>', 24), ('dor</w>', 1), ('ear</w>', 2), ('ece</w>', 4), ('ech</w>', 2), ('eld</w>', 5), ('ele</w>', 1), ('eli</w>', 3), ('els</w>', 3), ('ene</w>', 1), ('ent</w>', 6), ('ere</w>', 2), ('ero</w>', 1), ('ers</w>', 9), ('fed</w>', 5), ('fem</w>', 1), ('fic</w>', 4), ('fit</w>', 8), ('for</w>', 209), ('foreclo', 2), ('fun</w>', 2), ('fur</w>', 1), ('ged</w>', 20), ('gel</w>', 1), ('ger</w>', 18), ('ges</w>', 12), ('get</w>', 26), ('ghanist', 4), ('ghi</w>', 1), ('ght</w>', 73), ('gic</w>', 2), ('gin</w>', 4), ('gis</w>', 1), ('god</w>', 1), ('gon</w>', 4), ('gor</w>', 1), ('gun</w>', 3), ('had</w>', 51), ('ham</w>', 2), ('han</w>', 2), ('har</w>', 1), ('has</w>', 72), ('hen</w>', 2), ('her</w>', 37), ('hic</w>', 1), ('hil</w>', 1), ('him</w>', 13), ('his</w>', 100), ('hit</w>', 4), ('hol</w>', 2), ('ice</w>', 7), ('ico</w>', 3), ('ild</w>', 4), ('ile</w>', 1), ('ill</w>', 3), ('ine</w>', 2), ('ing</w>', 98), ('ire</w>', 8), ('ise</w>', 3), ('ish</w>', 4), ('isi</w>', 1), ('iss</w>', 1), ('isy</w>', 1), ('ite</w>', 9), ('ith</w>', 2), ('its</w>', 50), ('ity</w>', 6), ('jon</w>', 1), ('jor</w>', 4), ('ked</w>', 27), ('ken</w>', 3), ('ker</w>', 6), ('ket</w>', 13), ('lan</w>', 2), ('led</w>', 42), ('lel</w>', 1), ('lem</w>', 3), ('len</w>', 5), ('ler</w>', 12), ('les</w>', 12), ('let</w>', 8), ('lex</w>', 1), ('lic</w>', 12), ('lin</w>', 2), ('lis</w>', 1), ('lon</w>', 1), ('man</w>', 48), ('mar</w>', 8), ('med</w>', 13), ('men</w>', 11), ('mer</w>', 19), ('mes</w>', 1), ('met</w>', 4), ('mex</w>', 1), ('mic</w>', 1), ('ministr', 4), ('mis</w>', 1), ('mit</w>', 2), ('mom</w>', 1), ('mon</w>', 4), ('mor</w>', 1), ('ned</w>', 18), ('nel</w>', 4), ('ner</w>', 16), ('nes</w>', 1), ('net</w>', 17), ('new</w>', 70), ('nil</w>', 2), ('non</w>', 8), ('not</w>', 56), ('old</w>', 17), ('ole</w>', 1), ('ols</w>', 6), ('omo</w>', 1), ('one</w>', 46), ('ons</w>', 44), ('our</w>', 42), ('ous</w>', 12), ('out</w>', 39), ('par</w>', 2), ('pec</w>', 2), ('ped</w>', 17), ('pen</w>', 7), ('per</w>', 21), ('perimen', 2), ('pes</w>', 4), ('pet</w>', 1), ('ple</w>', 11), ('pli</w>', 1), ('ply</w>', 8), ('pod</w>', 1), ('pol</w>', 1), ('politic', 3), ('pro</w>', 1), ('pronoun', 1), ('pur</w>', 1), ('pus</w>', 3), ('ran</w>', 10), ('red</w>', 9), ('rel</w>', 4), ('ren</w>', 15), ('res</w>', 2), ('ric</w>', 2), ('ril</w>', 3), ('ris</w>', 7), ('rit</w>', 1), ('ron</w>', 2), ('ror</w>', 4), ('run</w>', 9), ('sch</w>', 2), ('sec</w>', 1), ('sed</w>', 49), ('sel</w>', 2), ('sen</w>', 7), ('ser</w>', 5), ('ses</w>', 26), ('set</w>', 9), ('sex</w>', 3), ('shamele', 1), ('she</w>', 36), ('shi</w>', 1), ('sic</w>', 9), ('sin</w>', 1), ('sis</w>', 5), ('sit</w>', 4), ('son</w>', 33), ('sor</w>', 2), ('stiliti', 1), ('sto</w>', 1), ('sts</w>', 19), ('sty</w>', 1), ('sun</w>', 1), ('sus</w>', 2), ('ted</w>', 101), ('tel</w>', 3), ('ten</w>', 16), ('ter</w>', 46), ('tes</w>', 11), ('tex</w>', 2), ('the</w>', 1218), ('thoriti', 6), ('tic</w>', 5), ('til</w>', 5), ('tim</w>', 2), ('tin</w>', 4), ('tis</w>', 2), ('tly</w>', 26), ('toleran', 1), ('tom</w>', 1), ('ton</w>', 19), ('tor</w>', 18), ('tre</w>', 2), ('tro</w>', 1), ('tus</w>', 1), ('two</w>', 33), ('ure</w>', 3), ('uro</w>', 2), ('ved</w>', 34), ('vel</w>', 7), ('ven</w>', 31), ('ver</w>', 38), ('veronic', 1), ('ves</w>', 25), ('vic</w>', 1), ('vicemem', 1), ('vil</w>', 5), ('vin</w>', 3), ('vis</w>', 1), ('von</w>', 2), ('vor</w>', 2), ('was</w>', 121), ('way</w>', 35), ('wed</w>', 18), ('wel</w>', 1), ('wen</w>', 2), ('wer</w>', 15), ('who</w>', 57), ('yed</w>', 4), ('yer</w>', 3), ('yet</w>', 2), ('you</w>', 48), (\"'s</w>\", 188), (\"'t</w>\", 46), ('ab</w>', 4), ('ac</w>', 4), ('acilit', 2), ('ad</w>', 60), ('af</w>', 1), ('ag</w>', 3), ('agricu', 1), ('ai</w>', 3), ('ak</w>', 5), ('al</w>', 218), ('am</w>', 39), ('americ', 7), ('an</w>', 175), ('ap</w>', 13), ('ar</w>', 38), ('as</w>', 163), ('astron', 1), ('at</w>', 134), ('atemen', 3), ('atholo', 2), ('atrici', 1), ('au</w>', 4), ('ay</w>', 52), ('be</w>', 100), ('bi</w>', 3), ('bo</w>', 1), ('busine', 23), ('by</w>', 106), ('calori', 4), ('ce</w>', 125), ('ch</w>', 57), ('charit', 1), ('chitec', 2), ('ci</w>', 2), ('co</w>', 7), ('coloni', 1), ('de</w>', 52), ('decisi', 2), ('di</w>', 3), ('disclo', 1), ('discon', 1), ('discre', 2), ('disper', 1), ('disten', 1), ('distin', 1), ('distri', 4), ('do</w>', 17), ('ds</w>', 101), ('du</w>', 1), ('dy</w>', 25), ('econom', 6), ('ed</w>', 65), ('el</w>', 22), ('elemen', 3), ('elimin', 2), ('em</w>', 2), ('en</w>', 32), ('er</w>', 33), ('es</w>', 153), ('et</w>', 25), ('ex</w>', 2), ('fe</w>', 17), ('ff</w>', 21), ('fferen', 4), ('fferin', 1), ('fisher', 1), ('ge</w>', 62), ('gh</w>', 54), ('gistic', 1), ('gn</w>', 29), ('go</w>', 18), ('gs</w>', 32), ('gy</w>', 26), ('ha</w>', 1), ('he</w>', 127), ('helico', 3), ('hemisp', 1), ('histor', 2), ('ho</w>', 3), ('homici', 1), ('ic</w>', 1), ('if</w>', 30), ('il</w>', 11), ('im</w>', 4), ('in</w>', 435), ('intere', 12), ('interi', 2), ('ir</w>', 2), ('iremen', 2), ('is</w>', 165), ('it</w>', 121), ('jo</w>', 1), ('ke</w>', 13), ('ks</w>', 41), ('ld</w>', 15), ('le</w>', 73), ('lo</w>', 3), ('ls</w>', 30), ('ly</w>', 75), ('me</w>', 22), ('militi', 2), ('miscom', 1), ('miscon', 1), ('mr</w>', 28), ('muniti', 2), ('ne</w>', 8), ('ni</w>', 4), ('nichol', 1), ('no</w>', 27), ('od</w>', 23), ('of</w>', 538), ('ol</w>', 13), ('om</w>', 11), ('on</w>', 225), ('or</w>', 63), ('pe</w>', 11), ('pearan', 1), ('perime', 1), ('pl</w>', 1), ('po</w>', 2), ('polici', 1), ('ps</w>', 29), ('punish', 1), ('re</w>', 17), ('ri</w>', 3), ('richar', 1), ('riteri', 1), ('ro</w>', 1), ('scener', 1), ('schaff', 1), ('se</w>', 115), ('sh</w>', 17), ('si</w>', 2), ('simili', 1), ('sisten', 1), ('so</w>', 24), ('sparen', 1), ('sparin', 1), ('ss</w>', 106), ('st</w>', 116), ('stican', 1), ('stimon', 2), ('su</w>', 1), ('sy</w>', 2), ('te</w>', 32), ('th</w>', 93), ('thorou', 1), ('ti</w>', 5), ('to</w>', 537), ('touris', 1), ('tr</w>', 1), ('ts</w>', 193), ('tu</w>', 1), ('ty</w>', 46), ('un</w>', 2), ('up</w>', 27), ('us</w>', 29), ('ve</w>', 110), ('vi</w>', 1), ('vo</w>', 2), ('we</w>', 65), ('wn</w>', 54), ('ws</w>', 31), ('ye</w>', 3), ('N</w>', 779), ('a</w>', 745), ('abili', 1), ('aditi', 5), ('affec', 1), ('affic', 3), ('affor', 2), ('afric', 2), ('amero', 1), ('amili', 4), ('amoun', 7), ('aneri', 1), ('apolo', 1), ('arene', 3), ('ariti', 1), ('aroun', 13), ('ascen', 1), ('ashin', 10), ('aspec', 1), ('astro', 1), ('astru', 3), ('athol', 1), ('atolo', 1), ('atric', 1), ('auren', 1), ('b</w>', 31), ('belon', 1), ('biche', 1), ('bilin', 1), ('bisec', 1), ('bisho', 2), ('c</w>', 23), ('celer', 2), ('cemen', 1), ('chair', 5), ('chene', 1), ('chine', 4), ('chiri', 1), ('cholo', 1), ('chore', 2), ('clear', 1), ('color', 1), ('d</w>', 201), ('decem', 2), ('decon', 1), ('delin', 1), ('deman', 4), ('demen', 1), ('demon', 3), ('direc', 13), ('disci', 2), ('disco', 2), ('discu', 2), ('displ', 2), ('dispu', 2), ('ditor', 1), ('e</w>', 103), ('enemi', 1), ('f</w>', 42), ('feren', 10), ('ffici', 16), ('fficu', 1), ('filin', 1), ('funer', 1), ('g</w>', 74), ('gemen', 1), ('gener', 11), ('gomer', 1), ('groun', 9), ('habit', 1), ('hemin', 1), ('hemis', 2), ('hemor', 1), ('herit', 1), ('hilan', 1), ('honor', 1), ('i</w>', 98), ('inclu', 23), ('inter', 30), ('inven', 3), ('inver', 1), ('irani', 1), ('irele', 2), ('juror', 2), ('k</w>', 196), ('l</w>', 84), ('lecor', 1), ('licen', 4), ('limit', 1), ('liter', 3), ('m</w>', 72), ('memor', 3), ('micha', 4), ('michi', 3), ('milit', 8), ('momen', 2), ('munic', 5), ('n</w>', 72), ('nomin', 2), ('noran', 1), ('o</w>', 38), ('oriti', 1), ('p</w>', 98), ('paren', 6), ('polic', 4), ('polis', 1), ('prece', 1), ('preci', 2), ('premi', 7), ('presi', 1), ('promo', 4), ('r</w>', 14), ('recal', 1), ('recen', 4), ('recom', 3), ('recon', 3), ('recor', 14), ('remin', 3), ('remis', 1), ('rimin', 2), ('ritic', 1), ('ritor', 1), ('roris', 2), ('s</w>', 229), ('scali', 1), ('schem', 1), ('schol', 1), ('score', 1), ('secon', 11), ('secre', 1), ('selec', 2), ('semin', 1), ('share', 4), ('simil', 6), ('solan', 1), ('speci', 25), ('stear', 1), ('stero', 1), ('stini', 3), ('stitu', 7), ('stran', 3), ('stren', 1), ('stric', 2), ('stron', 3), ('suran', 5), ('t</w>', 306), ('terin', 1), ('there', 1), ('tholo', 1), ('thori', 2), ('toler', 1), ('torol', 1), ('toron', 2), ('trole', 1), ('u</w>', 31), ('unori', 1), ('urani', 2), ('urolo', 1), ('vemen', 1), ('vinen', 1), ('viron', 3), ('visor', 2), ('w</w>', 145), ('whate', 4), ('whene', 2), ('where', 1), ('whole', 1), ('x</w>', 33), ('y</w>', 255), ('</w>', 83), ('abel', 2), ('abor', 1), ('abun', 1), ('acan', 1), ('achi', 2), ('acho', 1), ('achu', 1), ('acis', 1), ('acou', 1), ('adel', 2), ('ader', 1), ('adic', 2), ('adil', 1), ('adon', 1), ('agen', 7), ('agit', 2), ('agre', 8), ('aile', 1), ('aine', 2), ('aist', 1), ('alan', 4), ('alen', 1), ('alex', 1), ('alin', 1), ('alis', 1), ('alit', 1), ('alon', 4), ('aman', 1), ('amen', 4), ('amer', 2), ('amin', 1), ('amon', 12), ('amor', 2), ('amou', 1), ('anec', 1), ('anim', 1), ('anin', 1), ('anis', 1), ('apar', 1), ('apel', 1), ('asci', 1), ('ashi', 3), ('asin', 1), ('ason', 2), ('aste', 1), ('aten', 1), ('ater', 3), ('athi', 1), ('atin', 1), ('atis', 3), ('ator', 10), ('atri', 2), ('atur', 8), ('aure', 1), ('avel', 1), ('aver', 3), ('avil', 1), ('avin', 1), ('ayer', 1), ('bech', 1), ('beli', 13), ('belo', 2), ('bene', 6), ('bere', 1), ('bitr', 1), ('boun', 2), ('bour', 1), ('bure', 1), ('buri', 1), ('cali', 4), ('cane', 1), ('care', 2), ('cele', 2), ('chal', 2), ('cham', 6), ('chan', 22), ('chap', 1), ('char', 13), ('chat', 1), ('chec', 5), ('chel', 2), ('chic', 5), ('chil', 13), ('chin', 10), ('chit', 1), ('chun', 1), ('chur', 5), ('cili', 2), ('cine', 1), ('cisi', 1), ('citi', 1), ('clel', 1), ('clem', 1), ('clic', 1), ('clim', 4), ('clin', 4), ('coun', 49), ('cour', 18), ('curi', 1), ('deci', 3), ('decl', 1), ('dele', 1), ('deli', 6), ('demo', 11), ('deni', 5), ('dere', 2), ('deri', 1), ('dici', 2), ('disc', 1), ('dise', 4), ('dist', 2), ('diti', 9), ('dole', 1), ('doli', 1), ('dome', 3), ('done', 1), ('echo', 1), ('elec', 8), ('elen', 1), ('emer', 2), ('emir', 2), ('emis', 2), ('ener', 4), ('enor', 1), ('enou', 6), ('erem', 1), ('eric', 1), ('feri', 1), ('fero', 1), ('ffec', 6), ('ffen', 2), ('ffer', 1), ('ffor', 12), ('fici', 2), ('fire', 1), ('fore', 10), ('foru', 1), ('gene', 2), ('geri', 4), ('gero', 1), ('gine', 4), ('gole', 1), ('gran', 5), ('grim', 1), ('grou', 2), ('hafe', 1), ('hage', 1), ('hair', 1), ('hali', 1), ('hame', 1), ('hani', 1), ('hati', 1), ('havi', 1), ('hear', 6), ('heli', 1), ('hili', 1), ('hilo', 2), ('hiro', 1), ('hisp', 1), ('home', 1), ('hour', 4), ('ican', 1), ('ilin', 1), ('inve', 24), ('isel', 1), ('item', 2), ('jour', 8), ('juni', 1), ('juri', 1), ('lani', 1), ('lear', 5), ('lech', 1), ('leri', 1), ('line', 3), ('lish', 3), ('lori', 5), ('mani', 2), ('manu', 5), ('mari', 1), ('maru', 1), ('mile', 3), ('mili', 1), ('minu', 9), ('miro', 1), ('mist', 4), ('misy', 1), ('mith', 1), ('miti', 1), ('mone', 11), ('mono', 2), ('moro', 4), ('moun', 2), ('muni', 1), ('muno', 1), ('nati', 3), ('near', 7), ('nece', 3), ('nici', 2), ('niti', 1), ('nolo', 16), ('noun', 10), ('olen', 3), ('omur', 1), ('onis', 1), ('pear', 2), ('pelo', 1), ('peri', 16), ('peru', 1), ('plan', 4), ('plen', 1), ('plic', 8), ('plin', 1), ('plom', 2), ('plon', 1), ('polo', 1), ('poun', 4), ('prem', 1), ('prom', 1), ('prou', 1), ('rece', 15), ('reco', 5), ('rele', 12), ('reli', 1), ('remo', 7), ('rene', 1), ('rich', 1), ('rone', 1), ('roni', 1), ('roun', 8), ('scan', 4), ('scel', 1), ('sche', 2), ('schi', 1), ('scho', 16), ('schu', 2), ('scor', 1), ('scre', 3), ('sear', 9), ('secu', 4), ('seni', 7), ('seri', 11), ('shab', 1), ('shar', 7), ('shel', 5), ('shen', 1), ('shir', 2), ('shor', 4), ('shou', 2), ('sici', 1), ('sili', 1), ('sist', 2), ('siti', 6), ('situ', 2), ('sole', 1), ('soli', 3), ('some', 8), ('soni', 1), ('soun', 3), ('sour', 9), ('spar', 2), ('spec', 13), ('spel', 1), ('spen', 8), ('spon', 8), ('spor', 8), ('spou', 1), ('spre', 2), ('spur', 1), ('star', 24), ('stem', 5), ('ster', 6), ('stim', 5), ('stin', 1), ('stir', 1), ('stit', 1), ('stor', 10), ('stre', 14), ('stri', 9), ('stru', 16), ('stun', 2), ('tech', 19), ('tecu', 1), ('tele', 4), ('teni', 1), ('tere', 1), ('teri', 2), ('than', 4), ('them', 4), ('ther', 10), ('thic', 1), ('thin', 10), ('thir', 4), ('thom', 8), ('thou', 23), ('thre', 9), ('thur', 10), ('tici', 3), ('ticu', 3), ('timi', 1), ('tinu', 16), ('tire', 1), ('titi', 2), ('titu', 1), ('tole', 1), ('toli', 1), ('tolo', 1), ('toni', 2), ('tori', 3), ('toru', 1), ('tour', 2), ('tran', 13), ('tren', 1), ('tric', 4), ('tril', 1), ('trol', 3), ('trou', 3), ('turi', 1), ('twor', 8), ('unem', 3), ('velo', 16), ('vene', 1), ('venu', 12), ('vere', 1), ('veri', 2), ('visi', 2), ('viti', 2), ('wech', 1), ('whit', 2), ('with', 1), ('year', 1), ('youn', 13), ('abe', 2), ('abo', 6), ('abu', 3), ('ace', 3), ('ach', 1), ('aci', 1), ('acu', 4), ('adi', 6), ('ado', 2), ('adu', 6), ('afe', 6), ('aff', 3), ('afi', 2), ('age', 2), ('agh', 9), ('agr', 1), ('agu', 9), ('aic', 1), ('ail', 12), ('aim', 3), ('ain', 26), ('air', 21), ('ais', 1), ('ake', 2), ('ale', 6), ('ali', 20), ('alo', 2), ('ame', 1), ('ami', 2), ('amu', 2), ('ane', 2), ('ani', 11), ('ano', 1), ('anu', 1), ('apu', 1), ('are', 15), ('ari', 9), ('aro', 2), ('asc', 2), ('ase', 3), ('ash', 4), ('asi', 6), ('asp', 1), ('ast', 2), ('ate', 14), ('ath', 12), ('ati', 23), ('ato', 2), ('atr', 1), ('atu', 2), ('aun', 6), ('aur', 3), ('avi', 12), ('avo', 3), ('aye', 2), ('bec', 24), ('bel', 4), ('ben', 11), ('ber', 13), ('bic', 1), ('bil', 14), ('bin', 1), ('bir', 8), ('bit', 3), ('bol', 2), ('bom', 10), ('bon', 5), ('bor', 9), ('bou', 1), ('bur', 11), ('cal', 23), ('can', 27), ('car', 37), ('cel', 5), ('cen', 41), ('cer', 20), ('cha', 3), ('che', 11), ('chi', 14), ('cho', 9), ('cil', 1), ('cin', 3), ('cir', 4), ('cle', 4), ('cli', 8), ('clo', 17), ('clu', 8), ('col', 15), ('com', 136), ('con', 167), ('cor', 25), ('cou', 5), ('cre', 36), ('cun', 2), ('cur', 21), ('dec', 1), ('del', 14), ('den', 30), ('der', 33), ('dic', 15), ('dil', 5), ('din', 15), ('dir', 3), ('dis', 13), ('dol', 5), ('dom', 3), ('don', 1), ('dor', 1), ('dou', 7), ('duc', 26), ('dun', 1), ('dur', 1), ('ear', 30), ('ecu', 9), ('ele', 3), ('eli', 3), ('eni', 2), ('eno', 1), ('ero', 3), ('fec', 4), ('fel', 7), ('fem', 4), ('fen', 13), ('fer', 10), ('ffe', 5), ('fic', 3), ('fil', 6), ('fin', 24), ('fir', 12), ('for', 92), ('fri', 24), ('fun', 14), ('fur', 8), ('gel', 1), ('gen', 10), ('ger', 10), ('ghe', 3), ('gho', 1), ('gin', 3), ('gir', 7), ('gis', 1), ('gol', 3), ('gon', 3), ('gor', 4), ('gre', 30), ('gri', 2), ('gro', 23), ('gru', 2), ('gun', 2), ('gur', 2), ('had', 2), ('hal', 6), ('ham', 9), ('han', 21), ('hap', 6), ('har', 10), ('has', 1), ('hat', 1), ('hau', 3), ('hav', 1), ('hay', 2), ('hec', 1), ('hel', 18), ('hem', 1), ('hen', 6), ('her', 4), ('hic', 1), ('hil', 3), ('him', 8), ('hin', 10), ('hir', 1), ('hol', 9), ('hom', 1), ('hon', 3), ('hor', 5), ('hou', 22), ('hun', 11), ('hur', 3), ('ice', 1), ('ici', 4), ('ine', 4), ('ini', 1), ('iri', 2), ('isi', 1), ('iso', 2), ('ist', 5), ('isu', 1), ('jec', 26), ('jon', 3), ('jor', 1), ('jun', 2), ('jur', 2), ('ken', 4), ('ker', 2), ('kno', 35), ('lan', 25), ('lec', 12), ('lem', 8), ('len', 5), ('lex', 2), ('lic', 1), ('lim', 2), ('lin', 8), ('lit', 7), ('lom', 2), ('lon', 16), ('lor', 1), ('lou', 1), ('man', 33), ('mar', 53), ('mel', 3), ('mem', 14), ('men', 24), ('mer', 12), ('mex', 3), ('mic', 2), ('mil', 4), ('min', 3), ('mir', 4), ('mis', 17), ('mit', 15), ('mol', 2), ('mon', 46), ('mor', 16), ('mou', 1), ('mur', 8), ('nec', 4), ('nel', 1), ('ner', 6), ('nex', 12), ('nin', 10), ('nis', 1), ('non', 2), ('nor', 21), ('nou', 1), ('ole', 3), ('olo', 5), ('oni', 1), ('ore', 9), ('ori', 2), ('oun', 18), ('our', 1), ('par', 67), ('pec', 14), ('pen', 27), ('peo', 1), ('per', 69), ('ple', 19), ('pli', 7), ('plo', 16), ('pol', 5), ('pon', 1), ('por', 36), ('pre', 46), ('pro', 156), ('pun', 1), ('pur', 12), ('ran', 36), ('rec', 3), ('rel', 18), ('rem', 11), ('ren', 9), ('ric', 6), ('ril', 1), ('rin', 12), ('ris', 3), ('rit', 5), ('rol', 8), ('rom', 2), ('ron', 11), ('ror', 2), ('rou', 28), ('run', 11), ('sch', 4), ('sci', 12), ('sco', 8), ('scu', 1), ('sec', 4), ('sel', 30), ('sem', 3), ('sen', 29), ('ser', 45), ('sex', 2), ('sha', 3), ('she', 3), ('shi', 26), ('sho', 44), ('shu', 1), ('sic', 2), ('sil', 2), ('sim', 5), ('sin', 25), ('sit', 2), ('sol', 11), ('som', 1), ('son', 7), ('sor', 7), ('sou', 13), ('spe', 8), ('spo', 15), ('ste', 24), ('sti', 23), ('sto', 13), ('str', 24), ('stu', 23), ('stw', 1), ('sun', 11), ('sur', 28), ('tec', 6), ('tel', 4), ('tem', 24), ('ten', 26), ('ter', 38), ('tex', 2), ('the', 22), ('thi', 2), ('tho', 19), ('thu', 1), ('tic', 5), ('tim', 6), ('tin', 9), ('tit', 7), ('tol', 2), ('tom', 5), ('ton', 8), ('tor', 21), ('tou', 10), ('tre', 11), ('tri', 34), ('tro', 16), ('tru', 5), ('tun', 1), ('tur', 32), ('twe', 16), ('uni', 24), ('unu', 2), ('uro', 10), ('vel', 3), ('vem', 5), ('ven', 12), ('ver', 78), ('vic', 10), ('vil', 10), ('vin', 6), ('vir', 1), ('vis', 4), ('vit', 2), ('vol', 8), ('vor', 1), ('wel', 17), ('wen', 4), ('wer', 1), ('whe', 7), ('who', 3), ('wor', 72), ('you', 4), ('ab', 33), ('ac', 156), ('ad', 76), ('af', 17), ('ag', 11), ('ai', 23), ('ak', 14), ('al', 168), ('am', 65), ('an', 196), ('ap', 98), ('ar', 172), ('as', 79), ('at', 60), ('au', 65), ('av', 26), ('ay', 33), ('be', 105), ('bi', 24), ('bo', 66), ('bu', 56), ('ce', 42), ('ch', 18), ('ci', 43), ('cl', 24), ('co', 46), ('cu', 54), ('de', 181), ('di', 97), ('do', 64), ('du', 25), ('el', 8), ('em', 16), ('en', 132), ('er', 15), ('ex', 111), ('fe', 63), ('ff', 6), ('fi', 51), ('fu', 41), ('ge', 40), ('gh', 44), ('gi', 44), ('go', 62), ('gr', 35), ('gu', 53), ('ha', 6), ('he', 49), ('hi', 45), ('ho', 54), ('hu', 29), ('ic', 14), ('il', 44), ('im', 38), ('in', 219), ('ir', 24), ('is', 27), ('it', 35), ('jo', 35), ('ju', 55), ('ke', 47), ('le', 145), ('li', 119), ('lo', 158), ('me', 63), ('mi', 28), ('mo', 63), ('mu', 43), ('ne', 107), ('ni', 38), ('no', 53), ('nu', 25), ('ol', 20), ('om', 5), ('on', 41), ('or', 60), ('ou', 31), ('pe', 44), ('pl', 59), ('po', 103), ('pu', 63), ('qu', 84), ('re', 318), ('ri', 121), ('ro', 124), ('ru', 59), ('sc', 13), ('se', 129), ('sh', 2), ('si', 125), ('so', 44), ('sp', 39), ('st', 138), ('su', 148), ('sy', 28), ('te', 62), ('th', 49), ('ti', 97), ('to', 101), ('tr', 94), ('tu', 25), ('tw', 6), ('un', 65), ('ur', 9), ('ve', 23), ('vi', 89), ('vo', 23), ('we', 70), ('wh', 9), ('ye', 11), (\"'\", 69), ('N', 38), ('a', 168), ('b', 419), ('c', 289), ('d', 323), ('e', 407), ('f', 323), ('g', 238), ('h', 53), ('i', 149), ('j', 63), ('k', 169), ('l', 316), ('m', 288), ('n', 117), ('o', 334), ('p', 489), ('q', 38), ('r', 189), ('s', 381), ('t', 389), ('u', 138), ('v', 88), ('w', 356), ('x', 26), ('y', 157), ('z', 111)]\n"
     ]
    }
   ],
   "source": [
    "training_iter_num = 300\n",
    "\n",
    "# 构建初始 BPE 词典\n",
    "training_bpe_vocab = build_bpe_vocab(training_corpus)\n",
    "\n",
    "for i in range(training_iter_num):\n",
    "    # 获取当前 BPE 词典中的所有 bigram 及其频数\n",
    "    bigram_freq = get_bigram_freq(training_bpe_vocab)\n",
    "\n",
    "    # 检查是否有 bigram 存在，否则停止训练\n",
    "    if not bigram_freq:\n",
    "        break\n",
    "    \n",
    "    # 找到出现频次最高的 bigram\n",
    "    most_frequent_bigram = max(bigram_freq, key=bigram_freq.get)\n",
    "    \n",
    "    # 根据该 bigram 更新 BPE 词典\n",
    "    training_bpe_vocab = refresh_bpe_vocab_by_merging_bigram(most_frequent_bigram, training_bpe_vocab)\n",
    "    # 输出当前迭代信息\n",
    "    print(f\"Iteration {i}: Merged bigram {most_frequent_bigram}\")\n",
    "\n",
    "# 获得最终的 BPE 分词\n",
    "training_bpe_tokens = get_bpe_tokens(training_bpe_vocab)\n",
    "print(\"Final BPE tokens:\", training_bpe_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea3ddd",
   "metadata": {},
   "source": [
    "测试BPE分词器的分词效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "c0cfdb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naturallanguageprocessing 的分词结果为：\n",
      "n atur al lan gu age pro ce s sing</w>\n"
     ]
    }
   ],
   "source": [
    "test_word = \"naturallanguageprocessing\"\n",
    "\n",
    "print(\"naturallanguageprocessing 的分词结果为：\")\n",
    "print_bpe_tokenize(test_word, training_bpe_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5f2a1",
   "metadata": {},
   "source": [
    "### 实验总结\n",
    "> TODO：请在这里填写实验总结。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
